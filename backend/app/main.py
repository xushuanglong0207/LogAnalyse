from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Body, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import os
from typing import List, Dict, Any, Optional
import json
from datetime import datetime, timedelta
from pydantic import BaseModel
import re
import uuid
from concurrent.futures import ThreadPoolExecutor
import bisect

# æš‚æ—¶æ³¨é‡Šæ‰æ•°æ®åº“ç›¸å…³å¯¼å…¥ï¼Œç­‰ä¾èµ–å®‰è£…å¥½åå†å¯ç”¨
# from .api.v1 import rules as rules_router

# å¯å­˜å‚¨å†…å®¹çš„æœ€å¤§å­—èŠ‚æ•°ï¼ˆé»˜è®¤20MBï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
MAX_CONTENT_BYTES = int(os.environ.get("MAX_CONTENT_BYTES", str(20 * 1024 * 1024)))

# ä¼šè¯æœ‰æ•ˆæœŸ
DEFAULT_TTL_HOURS = 24
REMEMBER_TTL_DAYS = 30
RETENTION_DAYS = int(os.environ.get("LOG_RETENTION_DAYS", "30"))  # æ—¥å¿—ä¿ç•™å¤©æ•°

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="æ—¥å¿—åˆ†æå¹³å° API",
    description="é«˜æ€§èƒ½çš„syslogå’Œkernlogæ—¥å¿—åˆ†æå¹³å°",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æš‚æ—¶æ³¨é‡Šæ‰APIè·¯ç”±æ³¨å†Œï¼Œç­‰ä¾èµ–å®‰è£…å¥½åå†å¯ç”¨
# app.include_router(rules_router.router, prefix="/api/v1", tags=["è§„åˆ™ç®¡ç†"])

# å†…å­˜å­˜å‚¨ï¼ˆä¸´æ—¶ï¼‰
uploaded_files: List[Dict[str, Any]] = []
analysis_results: List[Dict[str, Any]] = []
problems: List[Dict[str, Any]] = []  # é—®é¢˜åº“ï¼š{id, title, url, error_type, created_at}

# ç®€å•åå°åˆ†æé˜Ÿåˆ—ï¼ˆçº¿ç¨‹æ± ï¼‰ï¼Œé¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯
EXECUTOR = ThreadPoolExecutor(max_workers=int(os.environ.get("ANALYSIS_WORKERS", "2")))
ANALYSIS_RUNNING = set()  # file_id é›†åˆï¼Œè¡¨ç¤ºæ­£åœ¨åˆ†æ

# â€”â€” è§„åˆ™DSLè§£æ â€”â€”
class _Ast:
    def __init__(self, op=None, left=None, right=None, value=None):
        self.op = op  # 'AND' 'OR' 'NOT' or None
        self.left = left
        self.right = right
        self.value = value  # phrase

_DEF_PRECEDENCE = {'!': 3, '&': 2, '|': 1}

def _tokenize(expr: str):
    if not expr:
        return []
    s = expr.replace('ï¼', '!')
    out = []
    i = 0
    n = len(s)
    while i < n:
        c = s[i]
        if c.isspace():
            i += 1
            continue
        if c in '()&|!':
            out.append(c)
            i += 1
            continue
        if c == '"':
            j = i + 1
            buf = []
            while j < n and s[j] != '"':
                buf.append(s[j])
                j += 1
            # è·³è¿‡ç»“æŸå¼•å·
            i = j + 1 if j < n and s[j] == '"' else j
            out.append(('PHRASE', ''.join(buf)))
            continue
        # æ™®é€šå•è¯ç›´åˆ°ç©ºç™½æˆ–è¿ç®—ç¬¦
        j = i
        buf = []
        while j < n and (not s[j].isspace()) and s[j] not in '()&|!':
            buf.append(s[j])
            j += 1
        out.append(('PHRASE', ''.join(buf)))
        i = j
    # æ’å…¥éšå¼ä¸ï¼ˆANDï¼‰ï¼šoperand åç´§è·Ÿ operand/!/( çš„æƒ…å†µ
    implicit = []
    def is_operand(tok):
        return isinstance(tok, tuple) and tok[0] == 'PHRASE' or tok == ')'
    def is_unary_or_open(tok):
        return tok == '!' or tok == '(' or (isinstance(tok, tuple) and tok[0] == 'PHRASE')
    for idx, tok in enumerate(out):
        if idx > 0:
            prev = out[idx-1]
            if (is_operand(prev) and (tok == '(' or tok == '!' or (isinstance(tok, tuple) and tok[0]=='PHRASE'))):
                implicit.append('&')
        implicit.append(tok)
    return implicit

def _to_rpn(tokens):
    # Shunting-yard ç®—æ³•
    output = []
    ops = []
    def prec(op):
        return _DEF_PRECEDENCE.get(op, 0)
    i = 0
    n = len(tokens)
    while i < n:
        tok = tokens[i]
        if isinstance(tok, tuple):  # PHRASE
            output.append(tok)
        elif tok == '!' :
            ops.append(tok)
        elif tok in ('&','|'):
            while ops and ops[-1] != '(' and prec(ops[-1]) >= prec(tok):
                output.append(ops.pop())
            ops.append(tok)
        elif tok == '(':
            ops.append(tok)
        elif tok == ')':
            while ops and ops[-1] != '(':
                output.append(ops.pop())
            if ops and ops[-1] == '(':
                ops.pop()
        i += 1
    while ops:
        output.append(ops.pop())
    return output

def _rpn_to_ast(rpn):
    st = []
    for tok in rpn:
        if isinstance(tok, tuple):
            st.append(_Ast(value=tok[1]))
        elif tok == '!':
            a = st.pop() if st else _Ast(value='')
            st.append(_Ast(op='NOT', left=a))
        elif tok in ('&','|'):
            b = st.pop() if st else _Ast(value='')
            a = st.pop() if st else _Ast(value='')
            st.append(_Ast(op=('AND' if tok=='&' else 'OR'), left=a, right=b))
    return st[-1] if st else None

def _eval_ast(ast: _Ast, text_lower: str) -> bool:
    if ast is None:
        return False
    if ast.op is None:
        phrase = (ast.value or '').lower()
        if phrase == '':
            return False
        result = phrase in text_lower
        print(f"    æ£€æŸ¥çŸ­è¯­ '{phrase}' åœ¨æ–‡æœ¬ä¸­: {result}")  # è°ƒè¯•ä¿¡æ¯
        return result
    if ast.op == 'NOT':
        return not _eval_ast(ast.left, text_lower)
    if ast.op == 'AND':
        left_result = _eval_ast(ast.left, text_lower)
        right_result = _eval_ast(ast.right, text_lower)
        final_result = left_result and right_result
        print(f"    ANDæ“ä½œ: {left_result} & {right_result} = {final_result}")  # è°ƒè¯•ä¿¡æ¯
        return final_result
    if ast.op == 'OR':
        left_result = _eval_ast(ast.left, text_lower)
        right_result = _eval_ast(ast.right, text_lower)
        final_result = left_result or right_result
        print(f"    ORæ“ä½œ: {left_result} | {right_result} = {final_result}")  # è°ƒè¯•ä¿¡æ¯
        return final_result
    return False

# â€”â€” è§„åˆ™åŒ¹é…é€»è¾‘ â€”â€”

# é¢„å¤„ç†å†…å®¹ï¼šæ‹†åˆ†è¡Œã€å°å†™ç¼“å­˜ã€æ¢è¡Œä½ç½®ç´¢å¼•
def _precompute_content(content: str):
    lines = content.split('\n')
    lines_lower = [ln.lower() for ln in lines]
    content_lower = content.lower()
    # è®°å½•æ¯ä¸ªæ¢è¡Œç¬¦åœ¨å†…å®¹ä¸­çš„åç§»ï¼Œç”¨äºå¿«é€Ÿè¡Œå·å®šä½
    newline_positions = []
    off = 0
    for ln in lines[:-1]:  # æœ€åä¸€è¡Œä¹‹åæ²¡æœ‰æ¢è¡Œç¬¦
        off += len(ln)
        newline_positions.append(off)
        off += 1  # '\n'
    return {"lines": lines, "lines_lower": lines_lower, "content_lower": content_lower, "newline_positions": newline_positions}

def _line_number_from_pos(pos: int, newline_positions: list[int]) -> int:
    # åŸºäºäºŒåˆ†æŸ¥æ‰¾å¿«é€Ÿå®šä½è¡Œå·ï¼ˆ1-basedï¼‰
    return bisect.bisect_right(newline_positions, max(0, pos)) + 1

DSL_CACHE: Dict[str, Any] = {}

def _compile_dsl(rule_id: Any, expr: str):
    key = f"{rule_id}:{expr}"
    c = DSL_CACHE.get(key)
    if c:
        return c
    tokens = _tokenize(expr)
    rpn = _to_rpn(tokens)
    ast = _rpn_to_ast(rpn)
    phrases = [t[1] for t in tokens if isinstance(t, tuple) and t[0] == 'PHRASE']
    c = {"tokens": tokens, "rpn": rpn, "ast": ast, "phrases": phrases}
    DSL_CACHE[key] = c
    return c

def evaluate_rule_matches(content: str, rule: Dict[str, Any], pre: Optional[Dict[str, Any]] = None) -> List[Any]:
    """æ ¹æ®è§„åˆ™è¿”å›åŒ¹é…åˆ—è¡¨ã€‚æ”¯æŒ DSL(| & ! () å’Œå¼•å·çŸ­è¯­)ï¼›
    è‹¥æœªæ£€æµ‹åˆ°DSLç¬¦å·ï¼Œåˆ™å›é€€åˆ°æ—§çš„ OR/AND/NOT/æ­£åˆ™ è¡Œä¸ºã€‚
    ç»“æœä»¥â€œè¿‘ä¼¼è¡Œçº§â€è¿”å›ï¼Œé¿å…é€å­—åŒ¹é…ã€‚
    """
    prectx = pre or _precompute_content(content)
    lines = prectx["lines"]
    lines_lower = prectx["lines_lower"]
    content_lower = prectx["content_lower"]
    # é¢„åˆ¤ DSL
    expr = ''
    
    if isinstance(rule.get('dsl'), str) and rule['dsl'].strip():
        expr = rule['dsl'].strip()
    else:
        # å…¼å®¹ï¼šå¦‚æœ patterns æ˜¯å•è¡Œè¡¨è¾¾å¼ä¸”åŒ…å« DSL è¿ç®—ç¬¦ï¼Œåˆ™å½“ä½œ DSL
        pats = rule.get('patterns')
        if isinstance(pats, list) and len(pats)==1 and isinstance(pats[0], str):
            cand = pats[0].strip()
            if any(ch in cand for ch in ['&','|','!','ï¼','(',')','"']):
                expr = cand
    
    if expr:
        compiled = _compile_dsl(rule.get('id','0'), expr)
        tokens = compiled["tokens"]
        ast = compiled["ast"]
        # æŒ‰è¡Œè¯„ä¼°
        matches = []
        offset = 0
        matched_lines = 0
        for idx, line_lower in enumerate(lines_lower):
            if _eval_ast(ast, line_lower):
                matched_lines += 1
                # ä»£è¡¨æ€§çš„å‘½ä¸­ä½ç½®ï¼šå–ä»»æ„çŸ­è¯­é¦–æ¬¡å‡ºç°
                pos = 0
                found = False
                for p in compiled["phrases"]:
                    pl = p.lower()
                    k = line_lower.find(pl)
                        if k >= 0:
                            pos = k
                            found = True
                            break
                start_index = offset + (pos if found else 0)
                end_index = start_index + (len(compiled["phrases"][0]) if (found and compiled["phrases"]) else max(1, len(lines[idx])))
                # æ„é€ ä¸€ä¸ªä¸æ­£åˆ™åŒ¹é…å¯¹è±¡ç±»ä¼¼çš„è½»é‡å¯¹è±¡
                class M:
                    def __init__(self, s, e, g):
                        self._s=s; self._e=e; self._g=g
                    def start(self): return self._s
                    def end(self): return self._e
                    def group(self): return lines[idx].strip()
                matches.append(M(start_index, end_index, lines[idx].strip()))
            offset += len(lines[idx]) + 1
        return matches

    # â€”â€” æ—§é€»è¾‘å›é€€ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰ â€”â€”
    patterns = rule.get("patterns", []) or []
    operator = (rule.get("operator") or "OR").upper()
    is_regex = bool(rule.get("is_regex", True))

    def find_matches(pat: str):
        if is_regex:
            return list(re.finditer(pat, content, re.IGNORECASE))
        else:
            # ä½¿ç”¨ä¸åŒºåˆ†å¤§å°å†™çš„å•ä¸ªæ¨¡å¼æœç´¢
            return list(re.finditer(re.escape(pat), content, re.IGNORECASE))

    # æ€§èƒ½ä¼˜åŒ–ï¼šOR æƒ…å†µå°½å¯èƒ½åˆå¹¶ä¸ºä¸€æ¬¡æ­£åˆ™æ‰«æ
    if operator == "OR" and patterns:
        try:
            if is_regex:
                union = "(?:" + ")|(?:".join(patterns) + ")"
                reg = re.compile(union, re.IGNORECASE)
            else:
                union = "|".join(re.escape(p) for p in patterns)
                reg = re.compile(union, re.IGNORECASE)
            flat = list(reg.finditer(content))
            return flat
        except Exception:
            pass

    all_lists = [find_matches(p) for p in patterns]

    if operator == "AND":
        return [lst[0] for lst in all_lists if lst] if all(len(lst) > 0 for lst in all_lists) else []
    elif operator == "NOT":
        # æ‰€æœ‰æ¨¡å¼éƒ½ä¸å‡ºç°æ‰ç®—å‘½ä¸­ï¼ˆè¿”å›ä¸€ä¸ªç©ºå ä½åŒ¹é…ï¼‰
        if all(len(lst) == 0 for lst in all_lists):
            class M:
                def start(self): return 0
                def end(self): return 0
                def group(self): return ""
            return [M()]
        return []
    else:  # OR
        flat = [m for lst in all_lists for m in lst]
        return flat

# â€”â€” æŒä¹…åŒ–è®¾ç½® â€”â€”
DATA_DIR = os.environ.get("LOG_ANALYZER_DATA", os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "database")))
FILES_DIR = os.path.join(DATA_DIR, "uploads")
INDEX_PATH = os.path.join(DATA_DIR, "uploads_index.json")
ANALYSIS_INDEX_PATH = os.path.join(DATA_DIR, "analysis_results.json")
PROBLEMS_PATH = os.path.join(DATA_DIR, "problems.json")
RULES_PATH = os.path.join(DATA_DIR, "detection_rules.json")  # æ–°å¢è§„åˆ™æŒä¹…åŒ–è·¯å¾„
USERS_PATH = os.path.join(DATA_DIR, "users.json")

os.makedirs(FILES_DIR, exist_ok=True)

# å¯åŠ¨æ—¶åŠ è½½ç´¢å¼•
try:
    if os.path.exists(INDEX_PATH):
        with open(INDEX_PATH, "r", encoding="utf-8") as f:
            uploaded_files = json.load(f)
    else:
        uploaded_files = []
except Exception:
    uploaded_files = []

# å¯åŠ¨æ—¶åŠ è½½åˆ†æç»“æœç´¢å¼•
try:
    if os.path.exists(ANALYSIS_INDEX_PATH):
        with open(ANALYSIS_INDEX_PATH, "r", encoding="utf-8") as f:
            analysis_results = json.load(f)
    else:
        analysis_results = []
except Exception:
    analysis_results = []

# å¯åŠ¨æ—¶åŠ è½½é—®é¢˜åº“
try:
    if os.path.exists(PROBLEMS_PATH):
        with open(PROBLEMS_PATH, "r", encoding="utf-8") as f:
            problems = json.load(f)
    else:
        problems = []
except Exception:
    problems = []

# å¯åŠ¨æ—¶åŠ è½½ç”¨æˆ·
try:
    if os.path.exists(USERS_PATH):
        with open(USERS_PATH, "r", encoding="utf-8") as f:
            users = json.load(f)
    else:
        # è‹¥æ— æ–‡ä»¶ï¼Œä¿æŒå†…ç½®admin
        users: List[Dict[str, Any]] = [
            {"id": 1, "username": "admin", "email": "", "role": "ç®¡ç†å‘˜", "password": "admin123", "position": "ç®¡ç†å‘˜"}
        ]
except Exception:
    users = [
        {"id": 1, "username": "admin", "email": "", "role": "ç®¡ç†å‘˜", "password": "admin123", "position": "ç®¡ç†å‘˜"}
    ]


def save_index():
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(INDEX_PATH, "w", encoding="utf-8") as f:
            json.dump(uploaded_files, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def save_analysis_index():
    try:
        with open(ANALYSIS_INDEX_PATH, "w", encoding="utf-8") as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def save_problems():
    try:
        with open(PROBLEMS_PATH, "w", encoding="utf-8") as f:
            json.dump(problems, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def save_rules():
    """ä¿å­˜æ£€æµ‹è§„åˆ™åˆ°æ–‡ä»¶"""
    try:
        with open(RULES_PATH, "w", encoding="utf-8") as f:
            json.dump(detection_rules, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

# æ–°å¢ï¼šä¿å­˜ç”¨æˆ·åˆ°æ–‡ä»¶

def save_users():
    try:
        with open(USERS_PATH, "w", encoding="utf-8") as f:
            json.dump(users, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def load_rules():
    """ä»æ–‡ä»¶åŠ è½½æ£€æµ‹è§„åˆ™"""
    global detection_rules
    try:
        if os.path.exists(RULES_PATH):
            with open(RULES_PATH, "r", encoding="utf-8") as f:
                loaded_rules = json.load(f)
                # åˆå¹¶å†…ç½®è§„åˆ™å’Œç”¨æˆ·è§„åˆ™ï¼Œé¿å…é‡å¤
                builtin_ids = {r["id"] for r in detection_rules}
                for rule in loaded_rules:
                    if rule["id"] not in builtin_ids:
                        detection_rules.append(rule)
    except Exception as e:
        print(f"åŠ è½½è§„åˆ™å¤±è´¥: {e}")

def purge_old_uploads():
    """æ¸…ç†è¶…è¿‡ä¿ç•™æœŸçš„æ—¥å¿—æ–‡ä»¶åŠåˆ†æç»“æœ"""
    global uploaded_files, analysis_results
    try:
        cutoff = datetime.now() - timedelta(days=RETENTION_DAYS)
        remain = []
        removed_ids = set()
        for f in uploaded_files:
            try:
                ts = datetime.fromisoformat(f.get("upload_time", ""))
            except Exception:
                ts = datetime.now()
            if ts < cutoff:
                removed_ids.add(f.get("id"))
                p = f.get("path")
                try:
                    if p and os.path.exists(p):
                        os.remove(p)
                except Exception:
                    pass
            else:
                remain.append(f)
        if removed_ids:
            uploaded_files = remain
            analysis_results = [r for r in analysis_results if r.get("file_id") not in removed_ids]
            save_index()
            save_analysis_index()
    except Exception:
        pass

# ç®€æ˜“ç”¨æˆ·æ¨¡å‹ä¸å†…å­˜ç”¨æˆ·è¡¨
class UserCreate(BaseModel):
    username: str
    email: Optional[str] = ""
    password: Optional[str] = ""  # æ¼”ç¤ºç”¨ï¼ŒæœªåŠ å¯†
    role: Optional[str] = "æ™®é€šç”¨æˆ·"
    position: Optional[str] = ""  # èŒä½

class UserUpdate(BaseModel):
    email: Optional[str] = None
    role: Optional[str] = None
    password: Optional[str] = None
    position: Optional[str] = None

class LoginPayload(BaseModel):
    username: str
    password: str
    remember: bool = False

class ChangePasswordPayload(BaseModel):
    old_password: str
    new_password: str


# åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡è¿‡æœŸæ¸…ç†ï¼Œé¿å…å¯¼å…¥é˜¶æ®µè°ƒç”¨
@app.on_event("startup")
async def _startup_cleanup():
    purge_old_uploads()
    load_rules()  # å¯åŠ¨æ—¶åŠ è½½ä¿å­˜çš„è§„åˆ™

# è§„åˆ™ä¸æ–‡ä»¶å¤¹æ¨¡å‹
class RuleCreate(BaseModel):
    name: str
    description: Optional[str] = ""
    enabled: bool = True
    patterns: List[str] = []  # æ­£åˆ™æˆ–å…³é”®å­—åˆ—è¡¨
    operator: Optional[str] = "OR"      # å…¼å®¹æ—§é€»è¾‘
    is_regex: Optional[bool] = True
    folder_id: Optional[int] = 1
    dsl: Optional[str] = None            # æ–°å¢ï¼šDSL è¡¨è¾¾å¼

class RuleUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    enabled: Optional[bool] = None
    patterns: Optional[List[str]] = None
    operator: Optional[str] = None
    is_regex: Optional[bool] = None
    folder_id: Optional[int] = None
    dsl: Optional[str] = None

rule_folders: List[Dict[str, Any]] = [
    {"id": 1, "name": "é»˜è®¤"}
]

# åŸºç¡€å†…ç½®è§„åˆ™ï¼ˆå°†è‡ªåŠ¨æ‰©å±•ä¸ºpatterns+operator+folder_idï¼‰
detection_rules = [
    {"id": 1, "name": "OOM Killer", "description": "å†…å­˜æº¢å‡ºæ£€æµ‹", "enabled": True, "pattern": "Out of memory|OOM killer"},
    {"id": 2, "name": "Kernel Panic", "description": "å†…æ ¸å´©æºƒæ£€æµ‹", "enabled": True, "pattern": "Kernel panic|kernel BUG"},
    {"id": 3, "name": "Segmentation Fault", "description": "æ®µé”™è¯¯æ£€æµ‹", "enabled": True, "pattern": "segfault|segmentation fault"},
    {"id": 4, "name": "Disk Space Error", "description": "ç£ç›˜ç©ºé—´ä¸è¶³", "enabled": True, "pattern": "No space left|disk full"},
    {"id": 5, "name": "Network Error", "description": "ç½‘ç»œè¿æ¥é”™è¯¯", "enabled": True, "pattern": "Network unreachable|Connection refused"},
    {"id": 6, "name": "File System Error", "description": "æ–‡ä»¶ç³»ç»Ÿé”™è¯¯", "enabled": True, "pattern": "I/O error|filesystem error"},
    {"id": 7, "name": "Authentication Error", "description": "è®¤è¯å¤±è´¥æ£€æµ‹", "enabled": True, "pattern": "authentication failed|login failed"}
]
# æ‰©å±•å†…ç½®è§„åˆ™ç»“æ„
for r in detection_rules:
    r["folder_id"] = 1
    r["patterns"] = [r.pop("pattern")] if "pattern" in r else []
    r["operator"] = "OR"
    r["is_regex"] = True

# è§„èŒƒåŒ–é—®é¢˜ç±»å‹ï¼šå°†å„ç§å†™æ³•æ˜ å°„ä¸ºè§„åˆ™å
def normalize_error_type(et: str) -> str:
    try:
        et_l = (et or "").strip()
        for rr in detection_rules:
            if et_l == rr.get("name"):
                return rr["name"]
            pats = rr.get("patterns", []) or []
            joined = "|".join(pats)
            if et_l == joined:
                return rr["name"]
            for p in pats:
                pl = (p or "").lower(); el = et_l.lower()
                if pl == el or pl in el or el in pl:
                    return rr["name"]
        return et_l
    except Exception:
        return et

# â€”â€” é—®é¢˜åº“æ¨¡å‹ â€”â€”
class ProblemCreate(BaseModel):
    title: str
    url: str
    error_type: str  # å…³è”çš„é”™è¯¯ç±»å‹ï¼ˆå¦‚ I/O errorã€OOM Killer ç­‰ï¼‰
    category: Optional[str] = ""

class ProblemUpdate(BaseModel):
    title: Optional[str] = None
    url: Optional[str] = None
    error_type: Optional[str] = None
    category: Optional[str] = None

# ç®€æ˜“ä»¤ç‰Œä¼šè¯å­˜å‚¨ï¼štoken -> {user_id, expiry}
sessions: Dict[str, Dict[str, Any]] = {}

def _public_user(u: Dict[str, Any]) -> Dict[str, Any]:
    return {k: u.get(k, "") for k in ["id", "username", "email", "role", "position"]}

def create_session(user_id: int, remember: bool) -> Dict[str, Any]:
    token = uuid.uuid4().hex
    expiry = datetime.utcnow() + (timedelta(days=REMEMBER_TTL_DAYS) if remember else timedelta(hours=DEFAULT_TTL_HOURS))
    sessions[token] = {"user_id": user_id, "expiry": expiry}
    return {"token": token, "expires_at": expiry.isoformat() + "Z"}

def require_auth(authorization: Optional[str] = Header(None)) -> Dict[str, Any]:
    if not authorization or not authorization.lower().startswith("bearer "):
        raise HTTPException(status_code=401, detail="æœªæˆæƒ")
    token = authorization.split(" ", 1)[1].strip()
    session = sessions.get(token)
    if not session:
        raise HTTPException(status_code=401, detail="æ— æ•ˆä»¤ç‰Œ")
    if datetime.utcnow() > session["expiry"]:
        sessions.pop(token, None)
        raise HTTPException(status_code=401, detail="ä»¤ç‰Œå·²è¿‡æœŸ")
    user = next((u for u in users if u["id"] == session["user_id"]), None)
    if not user:
        raise HTTPException(status_code=401, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    return {"token": token, "user": user}

@app.get("/api/debug-rule")
async def debug_dsl_rule(ctx: Dict[str, Any] = Depends(require_auth)):
    """è°ƒè¯•DSLè§„åˆ™åŒ¹é…"""
    
    # æµ‹è¯•æ—¥å¿—å†…å®¹ï¼ˆä»æ‚¨çš„æˆªå›¾ä¸­æå–ï¼‰
    test_content = """aq_ring_rx_clean+0x175/0x560 [atlantic]
aq_ring_rx_clean+0x14d/0x560 [atlantic]
aq_ring_update_queue_state+0xd0/0x60 [atlantic]"""
    
    # æŸ¥æ‰¾æ‚¨çš„ä¸‡å…½ç½‘å¡è§„åˆ™
    your_rule = None
    for rule in detection_rules:
        if "ä¸‡å…½" in rule.get("name", "") or "atlantic" in rule.get("dsl", ""):
            your_rule = rule
            break
    
    if not your_rule:
        return {"error": "æœªæ‰¾åˆ°ä¸‡å…½ç½‘å¡è§„åˆ™", "all_rules": [{"id": r["id"], "name": r["name"], "dsl": r.get("dsl")} for r in detection_rules]}
    
    # æµ‹è¯•DSLè§„åˆ™
    debug_info = {
        "rule_info": {
            "id": your_rule["id"],
            "name": your_rule["name"],
            "dsl": your_rule.get("dsl"),
            "enabled": your_rule.get("enabled"),
            "patterns": your_rule.get("patterns")
        },
        "test_content": test_content,
        "content_contains_aq_ring": "aq_ring_rx_clean" in test_content.lower(),
        "content_contains_atlantic": "atlantic" in test_content.lower()
    }
    
    # ä½¿ç”¨evaluate_rule_matchesæµ‹è¯•
    try:
        matches = evaluate_rule_matches(test_content, your_rule)
        debug_info["matches_found"] = len(matches)
        debug_info["matches_details"] = []
        for i, match in enumerate(matches[:3]):
            if hasattr(match, 'group'):
                debug_info["matches_details"].append({
                    "index": i,
                    "text": match.group(),
                    "start": match.start(),
                    "end": match.end()
                })
            else:
                debug_info["matches_details"].append({
                    "index": i,
                    "match": str(match)
                })
    except Exception as e:
        debug_info["error"] = str(e)
        debug_info["matches_found"] = 0
    
    # æ‰‹åŠ¨æµ‹è¯•DSLé€»è¾‘
    try:
        # é¢„åˆ¤ DSL
        expr = ''
        if isinstance(your_rule.get('dsl'), str) and your_rule['dsl'].strip():
            expr = your_rule['dsl'].strip()
            debug_info["dsl_expression_found"] = expr
        else:
            # å…¼å®¹ï¼šå¦‚æœ patterns æ˜¯å•è¡Œè¡¨è¾¾å¼ä¸”åŒ…å« DSL è¿ç®—ç¬¦
            pats = your_rule.get('patterns')
            if isinstance(pats, list) and len(pats)==1 and isinstance(pats[0], str):
                cand = pats[0].strip()
                if any(ch in cand for ch in ['&','|','!','ï¼','(',')','"']):
                    expr = cand
                    debug_info["dsl_from_patterns"] = expr
        
        if expr:
            debug_info["parsing_dsl"] = True
            tokens = _tokenize(expr)
            debug_info["tokens"] = [str(t) for t in tokens]
            
            rpn = _to_rpn(tokens)
            debug_info["rpn"] = [str(r) for r in rpn]
            
            ast = _rpn_to_ast(rpn)
            debug_info["ast_created"] = str(ast) if ast else "None"
            
            # æŒ‰è¡Œæµ‹è¯•
            lines = test_content.split('\n')
            line_results = []
            for idx, line in enumerate(lines):
                line_lower = line.lower()
                line_match = _eval_ast(ast, line_lower)
                line_results.append({
                    "line_num": idx + 1,
                    "line_content": line,
                    "matched": line_match
                })
            debug_info["line_by_line_results"] = line_results
        else:
            debug_info["no_dsl_expression"] = True
            
    except Exception as e:
        debug_info["dsl_parse_error"] = str(e)
    
    return debug_info

@app.get("/api/test-dsl")
async def test_dsl_rule(rule: str, text: str, ctx: Dict[str, Any] = Depends(require_auth)):
    """æµ‹è¯•DSLè§„åˆ™åŠŸèƒ½"""
    try:
        # æ„å»ºè§„åˆ™å¯¹è±¡
        test_rule = {
            "dsl": rule,
            "enabled": True
        }
        
        # ä½¿ç”¨ç°æœ‰çš„evaluate_rule_matcheså‡½æ•°
        matches = evaluate_rule_matches(text, test_rule)
        
        return {
            "rule": rule,
            "text": text[:200] + "..." if len(text) > 200 else text,
            "matched": len(matches) > 0,
            "match_count": len(matches),
            "matches": [{"position": m.start() if hasattr(m, 'start') else 0, 
                        "text": m.group() if hasattr(m, 'group') else str(m)} for m in matches[:5]]
        }
    except Exception as e:
        return {
            "rule": rule,
            "text": text[:200] + "..." if len(text) > 200 else text,
            "matched": False,
            "error": str(e)
        }

@app.get("/")
async def root():
    return {
        "message": "ğŸš€ æ—¥å¿—åˆ†æå¹³å° API",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "backend"
    }

# è®¤è¯ä¸ç”¨æˆ·
@app.post("/api/auth/login")
async def login(payload: LoginPayload):
    user = next((u for u in users if u["username"].lower() == payload.username.lower()), None)
    if not user or user.get("password") != payload.password:
        raise HTTPException(status_code=401, detail="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
    session_info = create_session(user["id"], payload.remember)
    return {"message": "ç™»å½•æˆåŠŸ", "user": _public_user(user), **session_info}

@app.get("/api/auth/me")
async def me(ctx: Dict[str, Any] = Depends(require_auth)):
    return {"user": _public_user(ctx["user"]) }

@app.post("/api/auth/logout")
async def logout(ctx: Dict[str, Any] = Depends(require_auth)):
    token = ctx["token"]
    sessions.pop(token, None)
    return {"message": "å·²é€€å‡º"}

@app.post("/api/auth/change_password")
async def change_password(payload: ChangePasswordPayload, ctx: Dict[str, Any] = Depends(require_auth)):
    user = ctx["user"]
    if user.get("password") != payload.old_password:
        raise HTTPException(status_code=400, detail="åŸå¯†ç ä¸æ­£ç¡®")
    user["password"] = payload.new_password
    return {"message": "å¯†ç å·²æ›´æ–°"}

# ä»ªè¡¨æ¿
@app.get("/api/dashboard/stats")
async def get_dashboard_stats(ctx: Dict[str, Any] = Depends(require_auth)):
    # è¯»å–æœ€æ–°ç´¢å¼•ï¼Œç¡®ä¿ä¸ç£ç›˜åŒæ­¥
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    user_id = ctx["user"]["id"]
    up_count = len(uploaded_files)
    try:
        if os.path.exists(INDEX_PATH):
            with open(INDEX_PATH, "r", encoding="utf-8") as f:
                idx = json.load(f)
                if is_admin:
                    up_count = len(idx)
                else:
                    up_count = len([x for x in idx if (x.get("owner_id", 1) == user_id)])
    except Exception:
        if not is_admin:
            up_count = len([x for x in uploaded_files if x.get("owner_id", 1) == user_id])
    detected = 0
    total_runs = 0
    try:
        if os.path.exists(ANALYSIS_INDEX_PATH):
            with open(ANALYSIS_INDEX_PATH, "r", encoding="utf-8") as f:
                arr = json.load(f)
                total_runs = len(arr)
                if is_admin:
                    detected = sum(len(r.get("issues", [])) for r in arr)
        else:
                    mine = [r for r in arr if r.get("owner_id", 1) == user_id]
                    detected = sum(len(r.get("issues", [])) for r in mine)
        else:
            total_runs = len(analysis_results)
            if is_admin:
            detected = sum(len(r.get("issues", [])) for r in analysis_results)
            else:
                mine = [r for r in analysis_results if r.get("owner_id", 1) == user_id]
                detected = sum(len(r.get("issues", [])) for r in mine)
    except Exception:
        total_runs = len(analysis_results)
        if is_admin:
        detected = sum(len(r.get("issues", [])) for r in analysis_results)
        else:
            mine = [r for r in analysis_results if r.get("owner_id", 1) == user_id]
            detected = sum(len(r.get("issues", [])) for r in mine)
    resp = {
        "uploaded_files": up_count,
        "detected_issues": detected,
        "detection_rules": len([rule for rule in detection_rules if rule["enabled"]]),
        "recent_activity": []
    }
    if is_admin:
        resp["total_analysis_runs"] = total_runs
    return resp

# æ—¥å¿—ç®¡ç†
@app.post("/api/logs/upload")
async def upload_log_file(file: UploadFile = File(...), ctx: Dict[str, Any] = Depends(require_auth)):
    try:
        # æ”¾å®½æ–‡ä»¶ç±»å‹é™åˆ¶ï¼šæ¥å—æ‰€æœ‰ç±»å‹
        content = await file.read()
        if len(content) > MAX_CONTENT_BYTES:
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ {int(MAX_CONTENT_BYTES/1024/1024)}MB")
        content_str = content.decode('utf-8', errors='ignore')
        file_id = (max([f["id"] for f in uploaded_files]) + 1) if uploaded_files else 1
        filename = file.filename
        save_path = os.path.join(FILES_DIR, f"{file_id}_{filename}")
        with open(save_path, "w", encoding="utf-8", errors="ignore") as fw:
            fw.write(content_str)
        file_info = {
            "id": file_id,
            "filename": filename,
            "size": len(content),
            "upload_time": datetime.now().isoformat(),
            "path": save_path,
            "status": "uploaded",
            "owner_id": ctx["user"]["id"],
        }
        uploaded_files.append(file_info)
        save_index()
        # ä¸Šä¼ åä¹Ÿæ¸…ç†ä¸€æ¬¡è¿‡æœŸæ•°æ®
        purge_old_uploads()
        return {"message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ", "file_id": file_info["id"], "filename": filename, "size": len(content)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.get("/api/logs")
async def get_uploaded_files(ctx: Dict[str, Any] = Depends(require_auth)):
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    user_id = ctx["user"]["id"]
    files = [
            {"id": f["id"], "filename": f["filename"], "size": f["size"], "upload_time": f["upload_time"], "status": f["status"]}
            for f in uploaded_files
        if is_admin or (f.get("owner_id", 1) == user_id)
        ]
    return {"files": files}

@app.get("/api/logs/{file_id}")
async def get_log_file(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    f = next((x for x in uploaded_files if x["id"] == file_id), None)
    if not f:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    if not is_admin and f.get("owner_id", 1) != ctx["user"]["id"]:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®è¯¥æ–‡ä»¶")
    content = ""
    try:
        with open(f.get("path"), "r", encoding="utf-8", errors="ignore") as fr:
            content = fr.read(MAX_CONTENT_BYTES)
    except Exception:
        content = f.get("content", "")  # å‘åå…¼å®¹
    return {
        "id": f["id"],
        "filename": f["filename"],
        "size": f["size"],
        "upload_time": f["upload_time"],
        "content": content
    }

@app.delete("/api/logs/{file_id}")
async def delete_log_file(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global uploaded_files, analysis_results
    target = next((f for f in uploaded_files if f["id"] == file_id), None)
    if not target:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    if not is_admin and target.get("owner_id", 1) != ctx["user"]["id"]:
        raise HTTPException(status_code=403, detail="æ— æƒåˆ é™¤è¯¥æ–‡ä»¶")
    uploaded_files = [f for f in uploaded_files if f["id"] != file_id]
    analysis_results = [r for r in analysis_results if r.get("file_id") != file_id]
    try:
        if target.get("path") and os.path.exists(target["path"]):
            os.remove(target["path"])
    except Exception:
        pass
    save_index()
    save_analysis_index()
    return {"message": "æ–‡ä»¶å·²åˆ é™¤"}

@app.get("/api/logs/{file_id}/preview")
async def preview_log_file(file_id: int, offset: int = 0, size: int = 512*1024, ctx: Dict[str, Any] = Depends(require_auth)):
    """æŒ‰å­—èŠ‚åç§»è¿”å›æ—¥å¿—ç‰‡æ®µï¼Œç”¨äºå¤§æ–‡ä»¶åˆ†ç‰‡é¢„è§ˆã€‚
    è¿”å›ï¼šchunk(å­—ç¬¦ä¸²)ã€offsetã€next_offsetã€eofã€total_sizeã€filename
    """
    try:
        f = next((x for x in uploaded_files if x["id"] == file_id), None)
        if not f:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        # æƒé™æ ¡éªŒ
        is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
        if not is_admin and f.get("owner_id", 1) != ctx["user"]["id"]:
            raise HTTPException(status_code=403, detail="æ— æƒé¢„è§ˆè¯¥æ–‡ä»¶")
        filename = f.get("filename") or str(file_id)
        # ç¡®å®šæ€»å¤§å°
        total_size = 0
        content_bytes = None
        if f.get("path") and os.path.exists(f["path"]):
            try:
                total_size = os.path.getsize(f["path"])
                size = max(1, min(int(size), 1024 * 1024))  # ä¸Šé™1MBæ¯æ¬¡
                offset = max(0, min(int(offset), total_size))
                with open(f["path"], "rb") as frb:
                    frb.seek(offset)
                    data = frb.read(size)
                next_offset = offset + len(data)
                eof = next_offset >= total_size
                chunk = data.decode("utf-8", errors="ignore")
                return {
                    "file_id": file_id,
                    "filename": filename,
                    "offset": offset,
                    "next_offset": next_offset,
                    "eof": eof,
                    "total_size": total_size,
                    "chunk": chunk
                }
            except Exception as e:
                # å›é€€åˆ°å†…å­˜å†…å®¹
                pass
        # å†…å­˜å†…å®¹å›é€€
        content_str = f.get("content", "")
        content_bytes = content_str.encode("utf-8", errors="ignore")
        total_size = len(content_bytes)
        size = max(1, min(int(size), 1024 * 1024))
        offset = max(0, min(int(offset), total_size))
        data = content_bytes[offset: offset + size]
        next_offset = offset + len(data)
        eof = next_offset >= total_size
        chunk = data.decode("utf-8", errors="ignore")
        return {
            "file_id": file_id,
            "filename": filename,
            "offset": offset,
            "next_offset": next_offset,
            "eof": eof,
            "total_size": total_size,
            "chunk": chunk
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„è§ˆå¤±è´¥: {e}")

class AnalyzeTextPayload(BaseModel):
    text: str
    filename: Optional[str] = "pasted.log"

@app.post("/api/logs/analyze_text")
async def analyze_text(payload: AnalyzeTextPayload, ctx: Dict[str, Any] = Depends(require_auth)):
    text_bytes = len(payload.text.encode("utf-8"))
    if text_bytes > MAX_CONTENT_BYTES:
        raise HTTPException(status_code=400, detail=f"æ–‡æœ¬å†…å®¹è¶…è¿‡é™åˆ¶ï¼Œæœ€å¤§ {int(MAX_CONTENT_BYTES/1024/1024)}MB")
    # ä¸´æ—¶å­˜å‚¨ä¸ºä¸€æ¡æ–‡ä»¶è®°å½•ï¼ˆä¸å†™å…¥ç£ç›˜ï¼‰
    file_info = {
        "id": len(uploaded_files) + 1,
        "filename": payload.filename,
        "size": text_bytes,
        "upload_time": datetime.now().isoformat(),
        "content": payload.text,
        "status": "uploaded",
        "owner_id": ctx["user"]["id"],
    }
    uploaded_files.append(file_info)
    # æ–‡æœ¬åˆ†æåŒæ ·èµ°åå°é˜Ÿåˆ—
    ANALYSIS_RUNNING.add(file_info["id"])
    def _task():
        try:
            _perform_analysis(file_info["id"])
        finally:
            ANALYSIS_RUNNING.discard(file_info["id"])
    EXECUTOR.submit(_task)
    return JSONResponse(status_code=202, content={"status": "accepted", "file_id": file_info["id"]})

# è§„åˆ™åŒ¹é…é€»è¾‘


def _perform_analysis(file_id: int):
    file_info = next((f for f in uploaded_files if f["id"] == file_id), None)
    if not file_info:
        return
    # ä»ç£ç›˜è¯»å–
    content = ""
    try:
        with open(file_info.get("path"), "r", encoding="utf-8", errors="ignore") as fr:
            chunks = []
            read = 0
            while read < MAX_CONTENT_BYTES:
                part = fr.read(min(1024 * 1024, MAX_CONTENT_BYTES - read))
                if not part:
                    break
                chunks.append(part)
                read += len(part)
            content = ''.join(chunks)
    except Exception:
        content = file_info.get("content", "")
    
    # åˆ†æ
    issues = []
    pre = _precompute_content(content)
    lines = pre["lines"]
    print(f"å¼€å§‹åˆ†ææ–‡ä»¶ {file_id}ï¼Œè§„åˆ™æ•°é‡: {len(detection_rules)}")
    
    for rule in detection_rules:
        matches = evaluate_rule_matches(content, rule, pre)
        
        if not matches:
            continue
            
        # å¯¹äºDSLè§„åˆ™æˆ–æœ‰å¤šä¸ªåŒ¹é…çš„æƒ…å†µï¼Œåˆå¹¶ä¸ºä¸€ä¸ªé—®é¢˜
        if rule.get('dsl') or len(matches) > 1:
            # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„ä¿¡æ¯
            match_details = []
            all_contexts = []
            line_numbers = []
            
            for m in matches:
                if m is None:
                    line_number = 1
                    context = '\n'.join(lines[:5])
                    matched_text = ""
                else:
                    line_number = _line_number_from_pos(m.start(), pre["newline_positions"]) 
                    context_start = max(0, line_number - 2)
                    context_end = min(len(lines), line_number + 1)
                    context = '\n'.join(lines[context_start:context_end])
                    matched_text = m.group()
                
                match_details.append({
                    "line": line_number,
                    "text": matched_text,
                    "context": context
                })
                line_numbers.append(line_number)
                all_contexts.append(f"è¡Œ {line_number}: {matched_text}")
            
            # åˆ›å»ºåˆå¹¶çš„é—®é¢˜æ¡ç›®
            combined_context = '\n\n'.join([f"åŒ¹é… {i+1} (è¡Œ {detail['line']}):\n{detail['context']}" 
                                          for i, detail in enumerate(match_details)])
            combined_matched_text = f"å…± {len(matches)} ä¸ªåŒ¹é…: " + "; ".join([detail['text'] for detail in match_details if detail['text']])
            
            issues.append({
                "rule_name": rule["name"],
                "description": rule.get("description", ""),
                "line_number": min(line_numbers) if line_numbers else 1,
                "matched_text": combined_matched_text,
                "context": combined_context,
                "match_count": len(matches),
                "severity": "high" if ("panic" in rule["name"].lower() or "oom" in rule["name"].lower()) else "medium"
            })
        else:
            # å•ä¸ªåŒ¹é…çš„ä¼ ç»Ÿå¤„ç†æ–¹å¼
            m = matches[0]
            if m is None:
                line_number = 1
                context = '\n'.join(lines[:5])
                matched_text = ""
            else:
                line_number = _line_number_from_pos(m.start(), pre["newline_positions"]) 
                context_start = max(0, line_number - 3)
                context_end = min(len(lines), line_number + 2)
                context = '\n'.join(lines[context_start:context_end])
                matched_text = m.group()
            issues.append({
                "rule_name": rule["name"],
                "description": rule.get("description", ""),
                "line_number": line_number,
                "matched_text": matched_text,
                "context": context,
                "severity": "high" if ("panic" in rule["name"].lower() or "oom" in rule["name"].lower()) else "medium"
            })
    
    print(f"åˆ†æå®Œæˆï¼Œæ€»é—®é¢˜æ•°: {len(issues)}")
    
    result = {
        "file_id": file_id,
        "filename": file_info["filename"],
        "analysis_time": datetime.now().isoformat(),
        "issues": issues,
        "summary": {
            "total_issues": len(issues),
            "high_severity": len([i for i in issues if i["severity"] == "high"]),
            "medium_severity": len([i for i in issues if i["severity"] == "medium"])
        },
        "owner_id": file_info.get("owner_id", 1)
    }
    global analysis_results
    analysis_results = [r for r in analysis_results if r.get("file_id") != file_id]
    analysis_results.append(result)
    save_analysis_index()

@app.post("/api/logs/{file_id}/analyze")
async def analyze_log_file(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    # é˜²æ­¢é‡å¤ç‚¹å‡»
    if file_id in ANALYSIS_RUNNING:
        return JSONResponse(status_code=202, content={"status": "running"})
    # æƒé™æ ¡éªŒ
    f = next((x for x in uploaded_files if x["id"] == file_id), None)
    if not f:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    if not is_admin and f.get("owner_id", 1) != ctx["user"]["id"]:
        raise HTTPException(status_code=403, detail="æ— æƒåˆ†æè¯¥æ–‡ä»¶")
    ANALYSIS_RUNNING.add(file_id)
    def _task():
        try:
            _perform_analysis(file_id)
        finally:
            ANALYSIS_RUNNING.discard(file_id)
    EXECUTOR.submit(_task)
    return JSONResponse(status_code=202, content={"status": "accepted"})

@app.get("/api/analysis/{file_id}/status")
async def get_analysis_status(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    f = next((x for x in uploaded_files if x["id"] == file_id), None)
    if not f:
        return {"status": "none"}
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    if not is_admin and f.get("owner_id", 1) != ctx["user"]["id"]:
        return {"status": "none"}
    exists = next((r for r in analysis_results if r.get("file_id") == file_id), None)
    if exists:
        return {"status": "ready"}
    if file_id in ANALYSIS_RUNNING:
        return {"status": "running"}
    return {"status": "none"}

# åˆ†æç»“æœæŸ¥è¯¢
@app.get("/api/analysis/results")
async def get_analysis_results(ctx: Dict[str, Any] = Depends(require_auth)):
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    user_id = ctx["user"]["id"]
    if is_admin:
    return {"results": analysis_results}
    # éç®¡ç†å‘˜æŒ‰ owner è¿‡æ»¤
    return {"results": [r for r in analysis_results if r.get("owner_id", 1) == user_id]}

@app.get("/api/analysis/{file_id}")
async def get_file_analysis_result(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    # æƒé™æ ¡éªŒåŸºäºæ–‡ä»¶å±ä¸»
    f = next((x for x in uploaded_files if x["id"] == file_id), None)
    if not f:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    is_admin = (str(ctx["user"].get("username", "")).lower() == "admin")
    if not is_admin and f.get("owner_id", 1) != ctx["user"]["id"]:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®åˆ†æç»“æœ")
    result = next((r for r in analysis_results if r.get("file_id") == file_id), None)
    if not result:
        raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
    return result

# è§„åˆ™ç®¡ç†ä¸æ–‡ä»¶å¤¹
@app.get("/api/rules")
async def get_detection_rules(query: Optional[str] = None, folder_id: Optional[int] = None, ctx: Dict[str, Any] = Depends(require_auth)):
    rules = detection_rules
    if folder_id is not None:
        rules = [r for r in rules if r.get("folder_id") == folder_id]
    if query:
        q = query.lower()
        rules = [r for r in rules if q in r["name"].lower() or q in r.get("description", "").lower()]
    return {"rules": rules}

@app.post("/api/rules")
async def create_rule(payload: RuleCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    new_id = (max([r["id"] for r in detection_rules]) + 1) if detection_rules else 1
    rule = {
        "id": new_id,
        "name": payload.name,
        "description": payload.description or "",
        "enabled": payload.enabled,
        "patterns": payload.patterns or [],
        "operator": (payload.operator or "OR").upper() if payload.operator is not None else "OR",
        "is_regex": bool(payload.is_regex) if payload.is_regex is not None else True,
        "folder_id": payload.folder_id or 1,
        "dsl": (payload.dsl or "").strip(),
    }
    detection_rules.append(rule)
    save_rules()  # ä¿å­˜è§„åˆ™
    return {"message": "è§„åˆ™åˆ›å»ºæˆåŠŸ", "rule": rule}

@app.put("/api/rules/{rule_id}")
async def update_detection_rule(rule_id: int, payload: RuleUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    rule = next((r for r in detection_rules if r["id"] == rule_id), None)
    if not rule:
        raise HTTPException(status_code=404, detail="è§„åˆ™ä¸å­˜åœ¨")
    for k, v in payload.dict(exclude_unset=True).items():
        if k == "operator" and v:
            rule[k] = v.upper()
        else:
            rule[k] = v
    save_rules()  # ä¿å­˜è§„åˆ™
    return {"message": "è§„åˆ™æ›´æ–°æˆåŠŸ", "rule": rule}

@app.delete("/api/rules/{rule_id}")
async def delete_rule(rule_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global detection_rules
    before = len(detection_rules)
    detection_rules = [r for r in detection_rules if r["id"] != rule_id]
    if len(detection_rules) == before:
        raise HTTPException(status_code=404, detail="è§„åˆ™ä¸å­˜åœ¨")
    save_rules()  # ä¿å­˜è§„åˆ™
    return {"message": "è§„åˆ™å·²åˆ é™¤"}

@app.get("/api/rule-folders")
async def list_rule_folders(ctx: Dict[str, Any] = Depends(require_auth)):
    # é™„å¸¦æ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹è§„åˆ™æ•°é‡
    folders = []
    for f in rule_folders:
        cnt = len([r for r in detection_rules if r.get("folder_id") == f["id"]])
        folders.append({**f, "count": cnt})
    return {"folders": folders}

class FolderCreate(BaseModel):
    name: str

class FolderUpdate(BaseModel):
    name: str

@app.post("/api/rule-folders")
async def create_folder(payload: FolderCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    new_id = (max([f["id"] for f in rule_folders]) + 1) if rule_folders else 1
    folder = {"id": new_id, "name": payload.name}
    rule_folders.append(folder)
    return {"message": "æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ", "folder": folder}

@app.put("/api/rule-folders/{folder_id}")
async def rename_folder(folder_id: int, payload: FolderUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    folder = next((f for f in rule_folders if f["id"] == folder_id), None)
    if not folder:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
    folder["name"] = payload.name
    return {"message": "å·²é‡å‘½å", "folder": folder}

@app.delete("/api/rule-folders/{folder_id}")
async def delete_folder(folder_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global rule_folders
    if folder_id == 1:
        raise HTTPException(status_code=400, detail="é»˜è®¤æ–‡ä»¶å¤¹ä¸å¯åˆ é™¤")
    folder = next((f for f in rule_folders if f["id"] == folder_id), None)
    if not folder:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
    # è§„åˆ™è¿ç§»åˆ°é»˜è®¤æ–‡ä»¶å¤¹
    for r in detection_rules:
        if r.get("folder_id") == folder_id:
            r["folder_id"] = 1
    rule_folders = [f for f in rule_folders if f["id"] != folder_id]
    return {"message": "æ–‡ä»¶å¤¹å·²åˆ é™¤ï¼Œè§„åˆ™å·²è¿ç§»åˆ°é»˜è®¤æ–‡ä»¶å¤¹"}

# â€”â€” é—®é¢˜åº“æ¥å£ â€”â€”
@app.get("/api/problems")
async def list_problems(error_type: Optional[str] = None, q: Optional[str] = None, category: Optional[str] = None, ctx: Dict[str, Any] = Depends(require_auth)):
    items = problems
    if error_type:
        items = [p for p in items if normalize_error_type(p.get("error_type") or "") == error_type]
    if category:
        items = [p for p in items if (p.get("category") or "") == category]
    if q:
        ql = q.lower()
        items = [p for p in items if ql in (p.get("title","" ).lower()) or ql in (p.get("url","" ).lower()) or ql in (p.get("error_type","" ).lower())]
    return {"problems": items}

@app.post("/api/problems")
async def create_problem(payload: ProblemCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    new = {
        "id": (max([p["id"] for p in problems]) + 1) if problems else 1,
        "title": payload.title,
        "url": payload.url,
        "error_type": normalize_error_type(payload.error_type),
        "category": payload.category or "",
        "created_at": datetime.now().isoformat()
    }
    problems.append(new)
    save_problems()
    return {"message": "å·²åˆ›å»º", "problem": new}

@app.put("/api/problems/{pid}")
async def update_problem(pid: int, payload: ProblemUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    pr = next((p for p in problems if p["id"] == pid), None)
    if not pr:
        raise HTTPException(status_code=404, detail="é—®é¢˜ä¸å­˜åœ¨")
    for k, v in payload.dict(exclude_unset=True).items():
        if k == "error_type" and v is not None:
            pr[k] = normalize_error_type(v)
        else:
            pr[k] = v
    save_problems()
    return {"message": "å·²æ›´æ–°", "problem": pr}

@app.delete("/api/problems/{pid}")
async def delete_problem(pid: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global problems
    before = len(problems)
    problems = [p for p in problems if p["id"] != pid]
    if len(problems) == before:
        raise HTTPException(status_code=404, detail="é—®é¢˜ä¸å­˜åœ¨")
    save_problems()
    return {"message": "å·²åˆ é™¤"}

@app.get("/api/problems/stats")
async def problem_stats(types: Optional[str] = None, ctx: Dict[str, Any] = Depends(require_auth)):
    # types: é€—å·åˆ†éš”çš„é”™è¯¯ç±»å‹ï¼›è‹¥ä¸ºç©ºåˆ™ç»Ÿè®¡å…¨éƒ¨
    wanted = None
    if types:
        wanted = set([t for t in types.split(',') if t])
    by_type: Dict[str, int] = {}
    for p in problems:
        et = normalize_error_type(p.get("error_type") or "")
        if wanted and et not in wanted:
            continue
        by_type[et] = by_type.get(et, 0) + 1
    total = sum(by_type.values()) if wanted else len(problems)
    return {"total": total, "type_count": len(by_type), "by_type": by_type}

# ç”¨æˆ·ç®¡ç†ï¼ˆæ¼”ç¤ºï¼šä»»ä½•ç™»å½•ç”¨æˆ·å¯è®¿é—®ï¼‰
@app.get("/api/users")
async def list_users(ctx: Dict[str, Any] = Depends(require_auth)):
    return {"users": [_public_user(u) for u in users]}

@app.post("/api/users")
async def create_user(payload: UserCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    if any(u["username"].lower() == payload.username.lower() for u in users):
        raise HTTPException(status_code=400, detail="ç”¨æˆ·åå·²å­˜åœ¨")
    new_user = {
        "id": (max([u["id"] for u in users]) + 1) if users else 1,
        "username": payload.username,
        "email": payload.email or "",
        "role": payload.role or "æ™®é€šç”¨æˆ·",
        "password": payload.password or "",
        "position": payload.position or ""
    }
    users.append(new_user)
    save_users()
    return {"message": "ç”¨æˆ·åˆ›å»ºæˆåŠŸ", "user": _public_user(new_user)}

@app.put("/api/users/{user_id}")
async def update_user(user_id: int, payload: UserUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    user = next((u for u in users if u["id"] == user_id), None)
    if not user:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    if payload.email is not None:
        user["email"] = payload.email
    if payload.role is not None:
        user["role"] = payload.role
    if payload.password is not None and payload.password != "":
        user["password"] = payload.password
    if payload.position is not None:
        user["position"] = payload.position
    save_users()
    return {"message": "ç”¨æˆ·æ›´æ–°æˆåŠŸ", "user": _public_user(user)}

@app.delete("/api/users/{user_id}")
async def delete_user(user_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global users
    before = len(users)
    users = [u for u in users if u["id"] != user_id]
    if len(users) == before:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    save_users()
    return {"message": "ç”¨æˆ·å·²åˆ é™¤"}

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    ) 