from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Body, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import os
from typing import List, Dict, Any, Optional
import json
from datetime import datetime, timedelta
from pydantic import BaseModel
import re
import uuid
from concurrent.futures import ThreadPoolExecutor

# æš‚æ—¶æ³¨é‡Šæ‰æ•°æ®åº“ç›¸å…³å¯¼å…¥ï¼Œç­‰ä¾èµ–å®‰è£…å¥½åå†å¯ç”¨
# from .api.v1 import rules as rules_router

# å¯å­˜å‚¨å†…å®¹çš„æœ€å¤§å­—èŠ‚æ•°ï¼ˆé»˜è®¤20MBï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
MAX_CONTENT_BYTES = int(os.environ.get("MAX_CONTENT_BYTES", str(20 * 1024 * 1024)))

# ä¼šè¯æœ‰æ•ˆæœŸ
DEFAULT_TTL_HOURS = 24
REMEMBER_TTL_DAYS = 30
RETENTION_DAYS = int(os.environ.get("LOG_RETENTION_DAYS", "30"))  # æ—¥å¿—ä¿ç•™å¤©æ•°

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="æ—¥å¿—åˆ†æå¹³å° API",
    description="é«˜æ€§èƒ½çš„syslogå’Œkernlogæ—¥å¿—åˆ†æå¹³å°",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æš‚æ—¶æ³¨é‡Šæ‰APIè·¯ç”±æ³¨å†Œï¼Œç­‰ä¾èµ–å®‰è£…å¥½åå†å¯ç”¨
# app.include_router(rules_router.router, prefix="/api/v1", tags=["è§„åˆ™ç®¡ç†"])

# å†…å­˜å­˜å‚¨ï¼ˆä¸´æ—¶ï¼‰
uploaded_files: List[Dict[str, Any]] = []
analysis_results: List[Dict[str, Any]] = []
problems: List[Dict[str, Any]] = []  # é—®é¢˜åº“ï¼š{id, title, url, error_type, created_at}

# ç®€å•åå°åˆ†æé˜Ÿåˆ—ï¼ˆçº¿ç¨‹æ± ï¼‰ï¼Œé¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯
EXECUTOR = ThreadPoolExecutor(max_workers=int(os.environ.get("ANALYSIS_WORKERS", "2")))
ANALYSIS_RUNNING = set()  # file_id é›†åˆï¼Œè¡¨ç¤ºæ­£åœ¨åˆ†æ

# â€”â€” è§„åˆ™DSLè§£æ â€”â€”
class _Ast:
    def __init__(self, op=None, left=None, right=None, value=None):
        self.op = op  # 'AND' 'OR' 'NOT' or None
        self.left = left
        self.right = right
        self.value = value  # phrase

_DEF_PRECEDENCE = {'!': 3, '&': 2, '|': 1}

def _tokenize(expr: str):
    if not expr:
        return []
    s = expr.replace('ï¼', '!')
    out = []
    i = 0
    n = len(s)
    while i < n:
        c = s[i]
        if c.isspace():
            i += 1
            continue
        if c in '()&|!':
            out.append(c)
            i += 1
            continue
        if c == '"':
            j = i + 1
            buf = []
            while j < n and s[j] != '"':
                buf.append(s[j])
                j += 1
            # è·³è¿‡ç»“æŸå¼•å·
            i = j + 1 if j < n and s[j] == '"' else j
            out.append(('PHRASE', ''.join(buf)))
            continue
        # æ™®é€šå•è¯ç›´åˆ°ç©ºç™½æˆ–è¿ç®—ç¬¦
        j = i
        buf = []
        while j < n and (not s[j].isspace()) and s[j] not in '()&|!':
            buf.append(s[j])
            j += 1
        out.append(('PHRASE', ''.join(buf)))
        i = j
    # æ’å…¥éšå¼ä¸ï¼ˆANDï¼‰ï¼šoperand åç´§è·Ÿ operand/!/( çš„æƒ…å†µ
    implicit = []
    def is_operand(tok):
        return isinstance(tok, tuple) and tok[0] == 'PHRASE' or tok == ')'
    def is_unary_or_open(tok):
        return tok == '!' or tok == '(' or (isinstance(tok, tuple) and tok[0] == 'PHRASE')
    for idx, tok in enumerate(out):
        if idx > 0:
            prev = out[idx-1]
            if (is_operand(prev) and (tok == '(' or tok == '!' or (isinstance(tok, tuple) and tok[0]=='PHRASE'))):
                implicit.append('&')
        implicit.append(tok)
    return implicit

def _to_rpn(tokens):
    # Shunting-yard ç®—æ³•
    output = []
    ops = []
    def prec(op):
        return _DEF_PRECEDENCE.get(op, 0)
    i = 0
    n = len(tokens)
    while i < n:
        tok = tokens[i]
        if isinstance(tok, tuple):  # PHRASE
            output.append(tok)
        elif tok == '!' :
            ops.append(tok)
        elif tok in ('&','|'):
            while ops and ops[-1] != '(' and prec(ops[-1]) >= prec(tok):
                output.append(ops.pop())
            ops.append(tok)
        elif tok == '(':
            ops.append(tok)
        elif tok == ')':
            while ops and ops[-1] != '(':
                output.append(ops.pop())
            if ops and ops[-1] == '(':
                ops.pop()
        i += 1
    while ops:
        output.append(ops.pop())
    return output

def _rpn_to_ast(rpn):
    st = []
    for tok in rpn:
        if isinstance(tok, tuple):
            st.append(_Ast(value=tok[1]))
        elif tok == '!':
            a = st.pop() if st else _Ast(value='')
            st.append(_Ast(op='NOT', left=a))
        elif tok in ('&','|'):
            b = st.pop() if st else _Ast(value='')
            a = st.pop() if st else _Ast(value='')
            st.append(_Ast(op=('AND' if tok=='&' else 'OR'), left=a, right=b))
    return st[-1] if st else None

def _eval_ast(ast: _Ast, text_lower: str) -> bool:
    if ast is None:
        return False
    if ast.op is None:
        phrase = (ast.value or '').lower()
        if phrase == '':
            return False
        result = phrase in text_lower
        print(f"    æ£€æŸ¥çŸ­è¯­ '{phrase}' åœ¨æ–‡æœ¬ä¸­: {result}")  # è°ƒè¯•ä¿¡æ¯
        return result
    if ast.op == 'NOT':
        return not _eval_ast(ast.left, text_lower)
    if ast.op == 'AND':
        left_result = _eval_ast(ast.left, text_lower)
        right_result = _eval_ast(ast.right, text_lower)
        final_result = left_result and right_result
        print(f"    ANDæ“ä½œ: {left_result} & {right_result} = {final_result}")  # è°ƒè¯•ä¿¡æ¯
        return final_result
    if ast.op == 'OR':
        left_result = _eval_ast(ast.left, text_lower)
        right_result = _eval_ast(ast.right, text_lower)
        final_result = left_result or right_result
        print(f"    ORæ“ä½œ: {left_result} | {right_result} = {final_result}")  # è°ƒè¯•ä¿¡æ¯
        return final_result
    return False

# â€”â€” è§„åˆ™åŒ¹é…é€»è¾‘ â€”â€”

def evaluate_rule_matches(content: str, rule: Dict[str, Any]) -> List[Any]:
    """æ ¹æ®è§„åˆ™è¿”å›åŒ¹é…åˆ—è¡¨ã€‚æ”¯æŒ DSL(| & ! () å’Œå¼•å·çŸ­è¯­)ï¼›
    è‹¥æœªæ£€æµ‹åˆ°DSLç¬¦å·ï¼Œåˆ™å›é€€åˆ°æ—§çš„ OR/AND/NOT/æ­£åˆ™ è¡Œä¸ºã€‚
    ç»“æœä»¥â€œè¿‘ä¼¼è¡Œçº§â€è¿”å›ï¼Œé¿å…é€å­—åŒ¹é…ã€‚
    """
    # é¢„åˆ¤ DSL
    expr = ''
    if isinstance(rule.get('dsl'), str) and rule['dsl'].strip():
        expr = rule['dsl'].strip()
    else:
        # å…¼å®¹ï¼šå¦‚æœ patterns æ˜¯å•è¡Œè¡¨è¾¾å¼ä¸”åŒ…å« DSL è¿ç®—ç¬¦ï¼Œåˆ™å½“ä½œ DSL
        pats = rule.get('patterns')
        if isinstance(pats, list) and len(pats)==1 and isinstance(pats[0], str):
            cand = pats[0].strip()
            if any(ch in cand for ch in ['&','|','!','ï¼','(',')','"']):
                expr = cand
    if expr:
        print(f"  å¼€å§‹DSLå¤„ç†ï¼Œè¡¨è¾¾å¼: {expr}")  # è°ƒè¯•ä¿¡æ¯
        tokens = _tokenize(expr)
        print(f"  è¯æ³•åˆ†æç»“æœ: {tokens}")  # è°ƒè¯•ä¿¡æ¯
        rpn = _to_rpn(tokens)
        print(f"  RPN: {rpn}")  # è°ƒè¯•ä¿¡æ¯
        ast = _rpn_to_ast(rpn)
        print(f"  AST: {ast}")  # è°ƒè¯•ä¿¡æ¯
        # æŒ‰è¡Œè¯„ä¼°
        lines = content.split('\n')
        matches = []
        offset = 0
        matched_lines = 0
        for idx, line in enumerate(lines):
            line_lower = line.lower()
            print(f"  æ£€æŸ¥ç¬¬{idx+1}è¡Œ: {line[:50]}...")  # è°ƒè¯•ä¿¡æ¯
            if _eval_ast(ast, line_lower):
                matched_lines += 1
                print(f"    âœ“ ç¬¬{idx+1}è¡ŒåŒ¹é…æˆåŠŸ!")  # è°ƒè¯•ä¿¡æ¯
                # æ‰¾åˆ°ä¸€ä¸ªä»£è¡¨æ€§çš„å‘½ä¸­ä½ç½®ï¼šå–ä»»æ„çŸ­è¯­é¦–æ¬¡å‡ºç°
                pos = 0
                found = False
                # ç²—ç•¥ä» tokens æå–çŸ­è¯­
                for t in tokens:
                    if isinstance(t, tuple) and t[0]=='PHRASE':
                        p = t[1].lower()
                        k = line_lower.find(p)
                        if k >= 0:
                            pos = k
                            found = True
                            break
                start_index = offset + (pos if found else 0)
                end_index = start_index + (len(tokens[0][1]) if found and isinstance(tokens[0], tuple) else max(1, len(line)))
                # æ„é€ ä¸€ä¸ªä¸æ­£åˆ™åŒ¹é…å¯¹è±¡ç±»ä¼¼çš„è½»é‡å¯¹è±¡
                class M:
                    def __init__(self, s, e, g):
                        self._s=s; self._e=e; self._g=g
                    def start(self): return self._s
                    def end(self): return self._e
                    def group(self): return self._g
                matches.append(M(start_index, end_index, line.strip()))
            offset += len(line) + 1
        
        print(f"  DSLåŒ¹é…å®Œæˆï¼Œå…±åŒ¹é… {matched_lines} è¡Œï¼Œè¿”å› {len(matches)} ä¸ªåŒ¹é…å¯¹è±¡")  # è°ƒè¯•ä¿¡æ¯
        return matches

    # â€”â€” æ—§é€»è¾‘å›é€€ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰ â€”â€”
    patterns = rule.get("patterns", []) or []
    operator = (rule.get("operator") or "OR").upper()
    is_regex = bool(rule.get("is_regex", True))

    def find_matches(pat: str):
        if is_regex:
            return list(re.finditer(pat, content, re.IGNORECASE))
        else:
            matches = []
            start = 0
            pat_l = pat.lower()
            cont_l = content.lower()
            while True:
                idx = cont_l.find(pat_l, start)
                if idx == -1:
                    break
                class M:
                    def __init__(self, s, e):
                        self._s = s; self._e = e
                    def start(self): return self._s
                    def end(self): return self._e
                    def group(self): return content[self._s:self._e]
                matches.append(M(idx, idx + len(pat)))
                start = idx + len(pat)
            return matches

    all_lists = [find_matches(p) for p in patterns]

    if operator == "AND":
        return [lst[0] for lst in all_lists if lst] if all(len(lst) > 0 for lst in all_lists) else []
    elif operator == "NOT":
        # æ‰€æœ‰æ¨¡å¼éƒ½ä¸å‡ºç°æ‰ç®—å‘½ä¸­ï¼ˆè¿”å›ä¸€ä¸ªç©ºå ä½åŒ¹é…ï¼‰
        if all(len(lst) == 0 for lst in all_lists):
            class M:
                def start(self): return 0
                def end(self): return 0
                def group(self): return ""
            return [M()]
        return []
    else:  # OR
        flat = [m for lst in all_lists for m in lst]
        return flat

# â€”â€” æŒä¹…åŒ–è®¾ç½® â€”â€”
DATA_DIR = os.environ.get("LOG_ANALYZER_DATA", os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "database")))
FILES_DIR = os.path.join(DATA_DIR, "uploads")
INDEX_PATH = os.path.join(DATA_DIR, "uploads_index.json")
ANALYSIS_INDEX_PATH = os.path.join(DATA_DIR, "analysis_results.json")
PROBLEMS_PATH = os.path.join(DATA_DIR, "problems.json")
RULES_PATH = os.path.join(DATA_DIR, "detection_rules.json")  # æ–°å¢è§„åˆ™æŒä¹…åŒ–è·¯å¾„

os.makedirs(FILES_DIR, exist_ok=True)

# å¯åŠ¨æ—¶åŠ è½½ç´¢å¼•
try:
    if os.path.exists(INDEX_PATH):
        with open(INDEX_PATH, "r", encoding="utf-8") as f:
            uploaded_files = json.load(f)
    else:
        uploaded_files = []
except Exception:
    uploaded_files = []

# å¯åŠ¨æ—¶åŠ è½½åˆ†æç»“æœç´¢å¼•
try:
    if os.path.exists(ANALYSIS_INDEX_PATH):
        with open(ANALYSIS_INDEX_PATH, "r", encoding="utf-8") as f:
            analysis_results = json.load(f)
    else:
        analysis_results = []
except Exception:
    analysis_results = []

# å¯åŠ¨æ—¶åŠ è½½é—®é¢˜åº“
try:
    if os.path.exists(PROBLEMS_PATH):
        with open(PROBLEMS_PATH, "r", encoding="utf-8") as f:
            problems = json.load(f)
    else:
        problems = []
except Exception:
    problems = []


def save_index():
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(INDEX_PATH, "w", encoding="utf-8") as f:
            json.dump(uploaded_files, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def save_analysis_index():
    try:
        with open(ANALYSIS_INDEX_PATH, "w", encoding="utf-8") as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def save_problems():
    try:
        with open(PROBLEMS_PATH, "w", encoding="utf-8") as f:
            json.dump(problems, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def save_rules():
    """ä¿å­˜æ£€æµ‹è§„åˆ™åˆ°æ–‡ä»¶"""
    try:
        with open(RULES_PATH, "w", encoding="utf-8") as f:
            json.dump(detection_rules, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def load_rules():
    """ä»æ–‡ä»¶åŠ è½½æ£€æµ‹è§„åˆ™"""
    global detection_rules
    try:
        if os.path.exists(RULES_PATH):
            with open(RULES_PATH, "r", encoding="utf-8") as f:
                loaded_rules = json.load(f)
                # åˆå¹¶å†…ç½®è§„åˆ™å’Œç”¨æˆ·è§„åˆ™ï¼Œé¿å…é‡å¤
                builtin_ids = {r["id"] for r in detection_rules}
                for rule in loaded_rules:
                    if rule["id"] not in builtin_ids:
                        detection_rules.append(rule)
    except Exception as e:
        print(f"åŠ è½½è§„åˆ™å¤±è´¥: {e}")

def purge_old_uploads():
    """æ¸…ç†è¶…è¿‡ä¿ç•™æœŸçš„æ—¥å¿—æ–‡ä»¶åŠåˆ†æç»“æœ"""
    global uploaded_files, analysis_results
    try:
        cutoff = datetime.now() - timedelta(days=RETENTION_DAYS)
        remain = []
        removed_ids = set()
        for f in uploaded_files:
            try:
                ts = datetime.fromisoformat(f.get("upload_time", ""))
            except Exception:
                ts = datetime.now()
            if ts < cutoff:
                removed_ids.add(f.get("id"))
                p = f.get("path")
                try:
                    if p and os.path.exists(p):
                        os.remove(p)
                except Exception:
                    pass
            else:
                remain.append(f)
        if removed_ids:
            uploaded_files = remain
            analysis_results = [r for r in analysis_results if r.get("file_id") not in removed_ids]
            save_index()
            save_analysis_index()
    except Exception:
        pass

# ç®€æ˜“ç”¨æˆ·æ¨¡å‹ä¸å†…å­˜ç”¨æˆ·è¡¨
class UserCreate(BaseModel):
    username: str
    email: Optional[str] = ""
    password: Optional[str] = ""  # æ¼”ç¤ºç”¨ï¼ŒæœªåŠ å¯†
    role: Optional[str] = "æ™®é€šç”¨æˆ·"
    position: Optional[str] = ""  # èŒä½

class UserUpdate(BaseModel):
    email: Optional[str] = None
    role: Optional[str] = None
    password: Optional[str] = None
    position: Optional[str] = None

class LoginPayload(BaseModel):
    username: str
    password: str
    remember: bool = False

class ChangePasswordPayload(BaseModel):
    old_password: str
    new_password: str

users: List[Dict[str, Any]] = [
    {"id": 1, "username": "admin", "email": "", "role": "ç®¡ç†å‘˜", "password": "admin123", "position": "ç®¡ç†å‘˜"}
]

# åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡è¿‡æœŸæ¸…ç†ï¼Œé¿å…å¯¼å…¥é˜¶æ®µè°ƒç”¨
@app.on_event("startup")
async def _startup_cleanup():
    purge_old_uploads()
    load_rules()  # å¯åŠ¨æ—¶åŠ è½½ä¿å­˜çš„è§„åˆ™

# è§„åˆ™ä¸æ–‡ä»¶å¤¹æ¨¡å‹
class RuleCreate(BaseModel):
    name: str
    description: Optional[str] = ""
    enabled: bool = True
    patterns: List[str] = []  # æ­£åˆ™æˆ–å…³é”®å­—åˆ—è¡¨
    operator: Optional[str] = "OR"      # å…¼å®¹æ—§é€»è¾‘
    is_regex: Optional[bool] = True
    folder_id: Optional[int] = 1
    dsl: Optional[str] = None            # æ–°å¢ï¼šDSL è¡¨è¾¾å¼

class RuleUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    enabled: Optional[bool] = None
    patterns: Optional[List[str]] = None
    operator: Optional[str] = None
    is_regex: Optional[bool] = None
    folder_id: Optional[int] = None
    dsl: Optional[str] = None

rule_folders: List[Dict[str, Any]] = [
    {"id": 1, "name": "é»˜è®¤"}
]

# åŸºç¡€å†…ç½®è§„åˆ™ï¼ˆå°†è‡ªåŠ¨æ‰©å±•ä¸ºpatterns+operator+folder_idï¼‰
detection_rules = [
    {"id": 1, "name": "OOM Killer", "description": "å†…å­˜æº¢å‡ºæ£€æµ‹", "enabled": True, "pattern": "Out of memory|OOM killer"},
    {"id": 2, "name": "Kernel Panic", "description": "å†…æ ¸å´©æºƒæ£€æµ‹", "enabled": True, "pattern": "Kernel panic|kernel BUG"},
    {"id": 3, "name": "Segmentation Fault", "description": "æ®µé”™è¯¯æ£€æµ‹", "enabled": True, "pattern": "segfault|segmentation fault"},
    {"id": 4, "name": "Disk Space Error", "description": "ç£ç›˜ç©ºé—´ä¸è¶³", "enabled": True, "pattern": "No space left|disk full"},
    {"id": 5, "name": "Network Error", "description": "ç½‘ç»œè¿æ¥é”™è¯¯", "enabled": True, "pattern": "Network unreachable|Connection refused"},
    {"id": 6, "name": "File System Error", "description": "æ–‡ä»¶ç³»ç»Ÿé”™è¯¯", "enabled": True, "pattern": "I/O error|filesystem error"},
    {"id": 7, "name": "Authentication Error", "description": "è®¤è¯å¤±è´¥æ£€æµ‹", "enabled": True, "pattern": "authentication failed|login failed"}
]
# æ‰©å±•å†…ç½®è§„åˆ™ç»“æ„
for r in detection_rules:
    r["folder_id"] = 1
    r["patterns"] = [r.pop("pattern")] if "pattern" in r else []
    r["operator"] = "OR"
    r["is_regex"] = True

# è§„èŒƒåŒ–é—®é¢˜ç±»å‹ï¼šå°†å„ç§å†™æ³•æ˜ å°„ä¸ºè§„åˆ™å
def normalize_error_type(et: str) -> str:
    try:
        et_l = (et or "").strip()
        for rr in detection_rules:
            if et_l == rr.get("name"):
                return rr["name"]
            pats = rr.get("patterns", []) or []
            joined = "|".join(pats)
            if et_l == joined:
                return rr["name"]
            for p in pats:
                pl = (p or "").lower(); el = et_l.lower()
                if pl == el or pl in el or el in pl:
                    return rr["name"]
        return et_l
    except Exception:
        return et

# â€”â€” é—®é¢˜åº“æ¨¡å‹ â€”â€”
class ProblemCreate(BaseModel):
    title: str
    url: str
    error_type: str  # å…³è”çš„é”™è¯¯ç±»å‹ï¼ˆå¦‚ I/O errorã€OOM Killer ç­‰ï¼‰
    category: Optional[str] = ""

class ProblemUpdate(BaseModel):
    title: Optional[str] = None
    url: Optional[str] = None
    error_type: Optional[str] = None
    category: Optional[str] = None

# ç®€æ˜“ä»¤ç‰Œä¼šè¯å­˜å‚¨ï¼štoken -> {user_id, expiry}
sessions: Dict[str, Dict[str, Any]] = {}

def _public_user(u: Dict[str, Any]) -> Dict[str, Any]:
    return {k: u.get(k, "") for k in ["id", "username", "email", "role", "position"]}

def create_session(user_id: int, remember: bool) -> Dict[str, Any]:
    token = uuid.uuid4().hex
    expiry = datetime.utcnow() + (timedelta(days=REMEMBER_TTL_DAYS) if remember else timedelta(hours=DEFAULT_TTL_HOURS))
    sessions[token] = {"user_id": user_id, "expiry": expiry}
    return {"token": token, "expires_at": expiry.isoformat() + "Z"}

def require_auth(authorization: Optional[str] = Header(None)) -> Dict[str, Any]:
    if not authorization or not authorization.lower().startswith("bearer "):
        raise HTTPException(status_code=401, detail="æœªæˆæƒ")
    token = authorization.split(" ", 1)[1].strip()
    session = sessions.get(token)
    if not session:
        raise HTTPException(status_code=401, detail="æ— æ•ˆä»¤ç‰Œ")
    if datetime.utcnow() > session["expiry"]:
        sessions.pop(token, None)
        raise HTTPException(status_code=401, detail="ä»¤ç‰Œå·²è¿‡æœŸ")
    user = next((u for u in users if u["id"] == session["user_id"]), None)
    if not user:
        raise HTTPException(status_code=401, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    return {"token": token, "user": user}

@app.get("/api/debug-rule")
async def debug_dsl_rule(ctx: Dict[str, Any] = Depends(require_auth)):
    """è°ƒè¯•DSLè§„åˆ™åŒ¹é…"""
    
    # æµ‹è¯•æ—¥å¿—å†…å®¹ï¼ˆä»æ‚¨çš„æˆªå›¾ä¸­æå–ï¼‰
    test_content = """aq_ring_rx_clean+0x175/0x560 [atlantic]
aq_ring_rx_clean+0x14d/0x560 [atlantic]
aq_ring_update_queue_state+0xd0/0x60 [atlantic]"""
    
    # æŸ¥æ‰¾æ‚¨çš„ä¸‡å…½ç½‘å¡è§„åˆ™
    your_rule = None
    for rule in detection_rules:
        if "ä¸‡å…½" in rule.get("name", "") or "atlantic" in rule.get("dsl", ""):
            your_rule = rule
            break
    
    if not your_rule:
        return {"error": "æœªæ‰¾åˆ°ä¸‡å…½ç½‘å¡è§„åˆ™", "all_rules": [{"id": r["id"], "name": r["name"], "dsl": r.get("dsl")} for r in detection_rules]}
    
    # æµ‹è¯•DSLè§„åˆ™
    debug_info = {
        "rule_info": {
            "id": your_rule["id"],
            "name": your_rule["name"],
            "dsl": your_rule.get("dsl"),
            "enabled": your_rule.get("enabled"),
            "patterns": your_rule.get("patterns")
        },
        "test_content": test_content,
        "content_contains_aq_ring": "aq_ring_rx_clean" in test_content.lower(),
        "content_contains_atlantic": "atlantic" in test_content.lower()
    }
    
    # ä½¿ç”¨evaluate_rule_matchesæµ‹è¯•
    try:
        matches = evaluate_rule_matches(test_content, your_rule)
        debug_info["matches_found"] = len(matches)
        debug_info["matches_details"] = []
        for i, match in enumerate(matches[:3]):
            if hasattr(match, 'group'):
                debug_info["matches_details"].append({
                    "index": i,
                    "text": match.group(),
                    "start": match.start(),
                    "end": match.end()
                })
            else:
                debug_info["matches_details"].append({
                    "index": i,
                    "match": str(match)
                })
    except Exception as e:
        debug_info["error"] = str(e)
        debug_info["matches_found"] = 0
    
    # æ‰‹åŠ¨æµ‹è¯•DSLé€»è¾‘
    try:
        # é¢„åˆ¤ DSL
        expr = ''
        if isinstance(your_rule.get('dsl'), str) and your_rule['dsl'].strip():
            expr = your_rule['dsl'].strip()
            debug_info["dsl_expression_found"] = expr
        else:
            # å…¼å®¹ï¼šå¦‚æœ patterns æ˜¯å•è¡Œè¡¨è¾¾å¼ä¸”åŒ…å« DSL è¿ç®—ç¬¦
            pats = your_rule.get('patterns')
            if isinstance(pats, list) and len(pats)==1 and isinstance(pats[0], str):
                cand = pats[0].strip()
                if any(ch in cand for ch in ['&','|','!','ï¼','(',')','"']):
                    expr = cand
                    debug_info["dsl_from_patterns"] = expr
        
        if expr:
            debug_info["parsing_dsl"] = True
            tokens = _tokenize(expr)
            debug_info["tokens"] = [str(t) for t in tokens]
            
            rpn = _to_rpn(tokens)
            debug_info["rpn"] = [str(r) for r in rpn]
            
            ast = _rpn_to_ast(rpn)
            debug_info["ast_created"] = str(ast) if ast else "None"
            
            # æŒ‰è¡Œæµ‹è¯•
            lines = test_content.split('\n')
            line_results = []
            for idx, line in enumerate(lines):
                line_lower = line.lower()
                line_match = _eval_ast(ast, line_lower)
                line_results.append({
                    "line_num": idx + 1,
                    "line_content": line,
                    "matched": line_match
                })
            debug_info["line_by_line_results"] = line_results
        else:
            debug_info["no_dsl_expression"] = True
            
    except Exception as e:
        debug_info["dsl_parse_error"] = str(e)
    
    return debug_info

@app.get("/api/test-dsl")
async def test_dsl_rule(rule: str, text: str, ctx: Dict[str, Any] = Depends(require_auth)):
    """æµ‹è¯•DSLè§„åˆ™åŠŸèƒ½"""
    try:
        # æ„å»ºè§„åˆ™å¯¹è±¡
        test_rule = {
            "dsl": rule,
            "enabled": True
        }
        
        # ä½¿ç”¨ç°æœ‰çš„evaluate_rule_matcheså‡½æ•°
        matches = evaluate_rule_matches(text, test_rule)
        
        return {
            "rule": rule,
            "text": text[:200] + "..." if len(text) > 200 else text,
            "matched": len(matches) > 0,
            "match_count": len(matches),
            "matches": [{"position": m.start() if hasattr(m, 'start') else 0, 
                        "text": m.group() if hasattr(m, 'group') else str(m)} for m in matches[:5]]
        }
    except Exception as e:
        return {
            "rule": rule,
            "text": text[:200] + "..." if len(text) > 200 else text,
            "matched": False,
            "error": str(e)
        }

@app.get("/")
async def root():
    return {
        "message": "ğŸš€ æ—¥å¿—åˆ†æå¹³å° API",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs"
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "backend"
    }

# è®¤è¯ä¸ç”¨æˆ·
@app.post("/api/auth/login")
async def login(payload: LoginPayload):
    user = next((u for u in users if u["username"].lower() == payload.username.lower()), None)
    if not user or user.get("password") != payload.password:
        raise HTTPException(status_code=401, detail="ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯")
    session_info = create_session(user["id"], payload.remember)
    return {"message": "ç™»å½•æˆåŠŸ", "user": _public_user(user), **session_info}

@app.get("/api/auth/me")
async def me(ctx: Dict[str, Any] = Depends(require_auth)):
    return {"user": _public_user(ctx["user"]) }

@app.post("/api/auth/logout")
async def logout(ctx: Dict[str, Any] = Depends(require_auth)):
    token = ctx["token"]
    sessions.pop(token, None)
    return {"message": "å·²é€€å‡º"}

@app.post("/api/auth/change_password")
async def change_password(payload: ChangePasswordPayload, ctx: Dict[str, Any] = Depends(require_auth)):
    user = ctx["user"]
    if user.get("password") != payload.old_password:
        raise HTTPException(status_code=400, detail="åŸå¯†ç ä¸æ­£ç¡®")
    user["password"] = payload.new_password
    return {"message": "å¯†ç å·²æ›´æ–°"}

# ä»ªè¡¨æ¿
@app.get("/api/dashboard/stats")
async def get_dashboard_stats(ctx: Dict[str, Any] = Depends(require_auth)):
    # è¯»å–æœ€æ–°ç´¢å¼•ï¼Œç¡®ä¿ä¸ç£ç›˜åŒæ­¥
    up_count = len(uploaded_files)
    try:
        if os.path.exists(INDEX_PATH):
            with open(INDEX_PATH, "r", encoding="utf-8") as f:
                up_count = len(json.load(f))
    except Exception:
        pass
    detected = 0
    try:
        if os.path.exists(ANALYSIS_INDEX_PATH):
            with open(ANALYSIS_INDEX_PATH, "r", encoding="utf-8") as f:
                detected = sum(len(r.get("issues", [])) for r in json.load(f))
        else:
            detected = sum(len(r.get("issues", [])) for r in analysis_results)
    except Exception:
        detected = sum(len(r.get("issues", [])) for r in analysis_results)
    return {
        "uploaded_files": up_count,
        "detected_issues": detected,
        "detection_rules": len([rule for rule in detection_rules if rule["enabled"]]),
        "recent_activity": []
    }

# æ—¥å¿—ç®¡ç†
@app.post("/api/logs/upload")
async def upload_log_file(file: UploadFile = File(...), ctx: Dict[str, Any] = Depends(require_auth)):
    try:
        # æ”¾å®½æ–‡ä»¶ç±»å‹é™åˆ¶ï¼šæ¥å—æ‰€æœ‰ç±»å‹
        content = await file.read()
        if len(content) > MAX_CONTENT_BYTES:
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ {int(MAX_CONTENT_BYTES/1024/1024)}MB")
        content_str = content.decode('utf-8', errors='ignore')
        file_id = (max([f["id"] for f in uploaded_files]) + 1) if uploaded_files else 1
        filename = file.filename
        save_path = os.path.join(FILES_DIR, f"{file_id}_{filename}")
        with open(save_path, "w", encoding="utf-8", errors="ignore") as fw:
            fw.write(content_str)
        file_info = {
            "id": file_id,
            "filename": filename,
            "size": len(content),
            "upload_time": datetime.now().isoformat(),
            "path": save_path,
            "status": "uploaded"
        }
        uploaded_files.append(file_info)
        save_index()
        # ä¸Šä¼ åä¹Ÿæ¸…ç†ä¸€æ¬¡è¿‡æœŸæ•°æ®
        purge_old_uploads()
        return {"message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ", "file_id": file_info["id"], "filename": filename, "size": len(content)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.get("/api/logs")
async def get_uploaded_files(ctx: Dict[str, Any] = Depends(require_auth)):
    return {
        "files": [
            {"id": f["id"], "filename": f["filename"], "size": f["size"], "upload_time": f["upload_time"], "status": f["status"]}
            for f in uploaded_files
        ]
    }

@app.get("/api/logs/{file_id}")
async def get_log_file(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    f = next((x for x in uploaded_files if x["id"] == file_id), None)
    if not f:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    content = ""
    try:
        with open(f.get("path"), "r", encoding="utf-8", errors="ignore") as fr:
            content = fr.read(MAX_CONTENT_BYTES)
    except Exception:
        content = f.get("content", "")  # å‘åå…¼å®¹
    return {
        "id": f["id"],
        "filename": f["filename"],
        "size": f["size"],
        "upload_time": f["upload_time"],
        "content": content
    }

@app.delete("/api/logs/{file_id}")
async def delete_log_file(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global uploaded_files, analysis_results
    target = next((f for f in uploaded_files if f["id"] == file_id), None)
    uploaded_files = [f for f in uploaded_files if f["id"] != file_id]
    analysis_results = [r for r in analysis_results if r.get("file_id") != file_id]
    if not target:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    try:
        if target.get("path") and os.path.exists(target["path"]):
            os.remove(target["path"])
    except Exception:
        pass
    save_index()
    save_analysis_index()
    return {"message": "æ–‡ä»¶å·²åˆ é™¤"}

class AnalyzeTextPayload(BaseModel):
    text: str
    filename: Optional[str] = "pasted.log"

@app.post("/api/logs/analyze_text")
async def analyze_text(payload: AnalyzeTextPayload, ctx: Dict[str, Any] = Depends(require_auth)):
    text_bytes = len(payload.text.encode("utf-8"))
    if text_bytes > MAX_CONTENT_BYTES:
        raise HTTPException(status_code=400, detail=f"æ–‡æœ¬å†…å®¹è¶…è¿‡é™åˆ¶ï¼Œæœ€å¤§ {int(MAX_CONTENT_BYTES/1024/1024)}MB")
    # ä¸´æ—¶å­˜å‚¨ä¸ºä¸€æ¡æ–‡ä»¶è®°å½•ï¼ˆä¸å†™å…¥ç£ç›˜ï¼‰
    file_info = {
        "id": len(uploaded_files) + 1,
        "filename": payload.filename,
        "size": text_bytes,
        "upload_time": datetime.now().isoformat(),
        "content": payload.text,
        "status": "uploaded"
    }
    uploaded_files.append(file_info)
    # æ–‡æœ¬åˆ†æåŒæ ·èµ°åå°é˜Ÿåˆ—
    ANALYSIS_RUNNING.add(file_info["id"])
    def _task():
        try:
            _perform_analysis(file_info["id"])
        finally:
            ANALYSIS_RUNNING.discard(file_info["id"])
    EXECUTOR.submit(_task)
    return JSONResponse(status_code=202, content={"status": "accepted", "file_id": file_info["id"]})

# è§„åˆ™åŒ¹é…é€»è¾‘

def evaluate_rule_matches(content: str, rule: Dict[str, Any]) -> List[Dict[str, Any]]:
    if not rule.get("enabled", True):
        return []
    patterns = rule.get("patterns", []) or []
    operator = (rule.get("operator") or "OR").upper()
    is_regex = bool(rule.get("is_regex", True))

    def find_matches(pat: str):
        if is_regex:
            return list(re.finditer(pat, content, re.IGNORECASE))
        else:
            matches = []
            start = 0
            pat_l = pat.lower()
            cont_l = content.lower()
            while True:
                idx = cont_l.find(pat_l, start)
                if idx == -1:
                    break
                class M:
                    def __init__(self, s, e):
                        self._s = s; self._e = e
                    def start(self): return self._s
                    def end(self): return self._e
                    def group(self): return content[self._s:self._e]
                matches.append(M(idx, idx + len(pat)))
                start = idx + len(pat)
            return matches

    all_lists = [find_matches(p) for p in patterns]

    if operator == "AND":
        # æ‰€æœ‰æ¨¡å¼è‡³å°‘å‡ºç°ä¸€æ¬¡æ‰ç®—åŒ¹é…ï¼Œè¿”å›æ¯ä¸ªæ¨¡å¼çš„é¦–ä¸ªåŒ¹é…
        if all(len(lst) > 0 for lst in all_lists):
            firsts = [lst[0] for lst in all_lists]
            return firsts
        return []
    elif operator == "NOT":
        # æ‰€æœ‰æ¨¡å¼éƒ½ä¸å‡ºç°æ‰ç®—å‘½ä¸­ï¼ˆè¿”å›ç©ºåŒ¹é…ä»¥æŒ‡ç¤ºå‘½ä¸­ï¼‰
        if all(len(lst) == 0 for lst in all_lists):
            return [None]  # ç”¨Noneå ä½è¡¨ç¤ºå‘½ä¸­ä½†æ— å…·ä½“ç‰‡æ®µ
        return []
    else:  # OR
        flat = [m for lst in all_lists for m in lst]
        return flat

def _perform_analysis(file_id: int):
    file_info = next((f for f in uploaded_files if f["id"] == file_id), None)
    if not file_info:
        return
    # ä»ç£ç›˜è¯»å–
    content = ""
    try:
        with open(file_info.get("path"), "r", encoding="utf-8", errors="ignore") as fr:
            chunks = []
            read = 0
            while read < MAX_CONTENT_BYTES:
                part = fr.read(min(1024 * 1024, MAX_CONTENT_BYTES - read))
                if not part:
                    break
                chunks.append(part)
                read += len(part)
            content = ''.join(chunks)
    except Exception:
        content = file_info.get("content", "")
    
    # åˆ†æ
    issues = []
    lines = content.split('\n')
    print(f"å¼€å§‹åˆ†ææ–‡ä»¶ {file_id}ï¼Œè§„åˆ™æ•°é‡: {len(detection_rules)}")
    
    for rule in detection_rules:
        print(f"æ£€æŸ¥è§„åˆ™: {rule['name']}, enabled: {rule.get('enabled', True)}, dsl: {rule.get('dsl', 'N/A')}")
        matches = evaluate_rule_matches(content, rule)
        print(f"  è§„åˆ™ {rule['name']} åŒ¹é…æ•°é‡: {len(matches)}")
        
        if not matches:
            continue
        for m in matches:
            if m is None:
                line_number = 1
                context = '\n'.join(lines[:5])
                matched_text = ""
            else:
                line_number = content[:m.start()].count('\n') + 1
                context_start = max(0, line_number - 3)
                context_end = min(len(lines), line_number + 2)
                context = '\n'.join(lines[context_start:context_end])
                matched_text = m.group()
            issues.append({
                "rule_name": rule["name"],
                "description": rule.get("description", ""),
                "line_number": line_number,
                "matched_text": matched_text,
                "context": context,
                "severity": "high" if ("panic" in rule["name"].lower() or "oom" in rule["name"].lower()) else "medium"
            })
    
    print(f"åˆ†æå®Œæˆï¼Œæ€»é—®é¢˜æ•°: {len(issues)}")
    
    result = {
        "file_id": file_id,
        "filename": file_info["filename"],
        "analysis_time": datetime.now().isoformat(),
        "issues": issues,
        "summary": {
            "total_issues": len(issues),
            "high_severity": len([i for i in issues if i["severity"] == "high"]),
            "medium_severity": len([i for i in issues if i["severity"] == "medium"])
        }
    }
    global analysis_results
    analysis_results = [r for r in analysis_results if r.get("file_id") != file_id]
    analysis_results.append(result)
    save_analysis_index()

@app.post("/api/logs/{file_id}/analyze")
async def analyze_log_file(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    # é˜²æ­¢é‡å¤ç‚¹å‡»
    if file_id in ANALYSIS_RUNNING:
        return JSONResponse(status_code=202, content={"status": "running"})
    ANALYSIS_RUNNING.add(file_id)
    def _task():
        try:
            _perform_analysis(file_id)
        finally:
            ANALYSIS_RUNNING.discard(file_id)
    EXECUTOR.submit(_task)
    return JSONResponse(status_code=202, content={"status": "accepted"})

@app.get("/api/analysis/{file_id}/status")
async def get_analysis_status(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    exists = next((r for r in analysis_results if r.get("file_id") == file_id), None)
    if exists:
        return {"status": "ready"}
    if file_id in ANALYSIS_RUNNING:
        return {"status": "running"}
    return {"status": "none"}

# åˆ†æç»“æœæŸ¥è¯¢
@app.get("/api/analysis/results")
async def get_analysis_results(ctx: Dict[str, Any] = Depends(require_auth)):
    return {"results": analysis_results}

@app.get("/api/analysis/{file_id}")
async def get_file_analysis_result(file_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    result = next((r for r in analysis_results if r.get("file_id") == file_id), None)
    if not result:
        raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")
    return result

# è§„åˆ™ç®¡ç†ä¸æ–‡ä»¶å¤¹
@app.get("/api/rules")
async def get_detection_rules(query: Optional[str] = None, folder_id: Optional[int] = None, ctx: Dict[str, Any] = Depends(require_auth)):
    rules = detection_rules
    if folder_id is not None:
        rules = [r for r in rules if r.get("folder_id") == folder_id]
    if query:
        q = query.lower()
        rules = [r for r in rules if q in r["name"].lower() or q in r.get("description", "").lower()]
    return {"rules": rules}

@app.post("/api/rules")
async def create_rule(payload: RuleCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    new_id = (max([r["id"] for r in detection_rules]) + 1) if detection_rules else 1
    rule = {
        "id": new_id,
        "name": payload.name,
        "description": payload.description or "",
        "enabled": payload.enabled,
        "patterns": payload.patterns or [],
        "operator": (payload.operator or "OR").upper() if payload.operator is not None else "OR",
        "is_regex": bool(payload.is_regex) if payload.is_regex is not None else True,
        "folder_id": payload.folder_id or 1,
        "dsl": (payload.dsl or "").strip(),
    }
    detection_rules.append(rule)
    save_rules()  # ä¿å­˜è§„åˆ™
    return {"message": "è§„åˆ™åˆ›å»ºæˆåŠŸ", "rule": rule}

@app.put("/api/rules/{rule_id}")
async def update_detection_rule(rule_id: int, payload: RuleUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    rule = next((r for r in detection_rules if r["id"] == rule_id), None)
    if not rule:
        raise HTTPException(status_code=404, detail="è§„åˆ™ä¸å­˜åœ¨")
    for k, v in payload.dict(exclude_unset=True).items():
        if k == "operator" and v:
            rule[k] = v.upper()
        else:
            rule[k] = v
    save_rules()  # ä¿å­˜è§„åˆ™
    return {"message": "è§„åˆ™æ›´æ–°æˆåŠŸ", "rule": rule}

@app.delete("/api/rules/{rule_id}")
async def delete_rule(rule_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global detection_rules
    before = len(detection_rules)
    detection_rules = [r for r in detection_rules if r["id"] != rule_id]
    if len(detection_rules) == before:
        raise HTTPException(status_code=404, detail="è§„åˆ™ä¸å­˜åœ¨")
    save_rules()  # ä¿å­˜è§„åˆ™
    return {"message": "è§„åˆ™å·²åˆ é™¤"}

@app.get("/api/rule-folders")
async def list_rule_folders(ctx: Dict[str, Any] = Depends(require_auth)):
    # é™„å¸¦æ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹è§„åˆ™æ•°é‡
    folders = []
    for f in rule_folders:
        cnt = len([r for r in detection_rules if r.get("folder_id") == f["id"]])
        folders.append({**f, "count": cnt})
    return {"folders": folders}

class FolderCreate(BaseModel):
    name: str

class FolderUpdate(BaseModel):
    name: str

@app.post("/api/rule-folders")
async def create_folder(payload: FolderCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    new_id = (max([f["id"] for f in rule_folders]) + 1) if rule_folders else 1
    folder = {"id": new_id, "name": payload.name}
    rule_folders.append(folder)
    return {"message": "æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ", "folder": folder}

@app.put("/api/rule-folders/{folder_id}")
async def rename_folder(folder_id: int, payload: FolderUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    folder = next((f for f in rule_folders if f["id"] == folder_id), None)
    if not folder:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
    folder["name"] = payload.name
    return {"message": "å·²é‡å‘½å", "folder": folder}

@app.delete("/api/rule-folders/{folder_id}")
async def delete_folder(folder_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global rule_folders
    if folder_id == 1:
        raise HTTPException(status_code=400, detail="é»˜è®¤æ–‡ä»¶å¤¹ä¸å¯åˆ é™¤")
    folder = next((f for f in rule_folders if f["id"] == folder_id), None)
    if not folder:
        raise HTTPException(status_code=404, detail="æ–‡ä»¶å¤¹ä¸å­˜åœ¨")
    # è§„åˆ™è¿ç§»åˆ°é»˜è®¤æ–‡ä»¶å¤¹
    for r in detection_rules:
        if r.get("folder_id") == folder_id:
            r["folder_id"] = 1
    rule_folders = [f for f in rule_folders if f["id"] != folder_id]
    return {"message": "æ–‡ä»¶å¤¹å·²åˆ é™¤ï¼Œè§„åˆ™å·²è¿ç§»åˆ°é»˜è®¤æ–‡ä»¶å¤¹"}

# â€”â€” é—®é¢˜åº“æ¥å£ â€”â€”
@app.get("/api/problems")
async def list_problems(error_type: Optional[str] = None, q: Optional[str] = None, category: Optional[str] = None, ctx: Dict[str, Any] = Depends(require_auth)):
    items = problems
    if error_type:
        items = [p for p in items if normalize_error_type(p.get("error_type") or "") == error_type]
    if category:
        items = [p for p in items if (p.get("category") or "") == category]
    if q:
        ql = q.lower()
        items = [p for p in items if ql in (p.get("title","" ).lower()) or ql in (p.get("url","" ).lower()) or ql in (p.get("error_type","" ).lower())]
    return {"problems": items}

@app.post("/api/problems")
async def create_problem(payload: ProblemCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    new = {
        "id": (max([p["id"] for p in problems]) + 1) if problems else 1,
        "title": payload.title,
        "url": payload.url,
        "error_type": normalize_error_type(payload.error_type),
        "category": payload.category or "",
        "created_at": datetime.now().isoformat()
    }
    problems.append(new)
    save_problems()
    return {"message": "å·²åˆ›å»º", "problem": new}

@app.put("/api/problems/{pid}")
async def update_problem(pid: int, payload: ProblemUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    pr = next((p for p in problems if p["id"] == pid), None)
    if not pr:
        raise HTTPException(status_code=404, detail="é—®é¢˜ä¸å­˜åœ¨")
    for k, v in payload.dict(exclude_unset=True).items():
        if k == "error_type" and v is not None:
            pr[k] = normalize_error_type(v)
        else:
            pr[k] = v
    save_problems()
    return {"message": "å·²æ›´æ–°", "problem": pr}

@app.delete("/api/problems/{pid}")
async def delete_problem(pid: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global problems
    before = len(problems)
    problems = [p for p in problems if p["id"] != pid]
    if len(problems) == before:
        raise HTTPException(status_code=404, detail="é—®é¢˜ä¸å­˜åœ¨")
    save_problems()
    return {"message": "å·²åˆ é™¤"}

@app.get("/api/problems/stats")
async def problem_stats(types: Optional[str] = None, ctx: Dict[str, Any] = Depends(require_auth)):
    # types: é€—å·åˆ†éš”çš„é”™è¯¯ç±»å‹ï¼›è‹¥ä¸ºç©ºåˆ™ç»Ÿè®¡å…¨éƒ¨
    wanted = None
    if types:
        wanted = set([t for t in types.split(',') if t])
    by_type: Dict[str, int] = {}
    for p in problems:
        et = normalize_error_type(p.get("error_type") or "")
        if wanted and et not in wanted:
            continue
        by_type[et] = by_type.get(et, 0) + 1
    total = sum(by_type.values()) if wanted else len(problems)
    return {"total": total, "type_count": len(by_type), "by_type": by_type}

# ç”¨æˆ·ç®¡ç†ï¼ˆæ¼”ç¤ºï¼šä»»ä½•ç™»å½•ç”¨æˆ·å¯è®¿é—®ï¼‰
@app.get("/api/users")
async def list_users(ctx: Dict[str, Any] = Depends(require_auth)):
    return {"users": [_public_user(u) for u in users]}

@app.post("/api/users")
async def create_user(payload: UserCreate, ctx: Dict[str, Any] = Depends(require_auth)):
    if any(u["username"].lower() == payload.username.lower() for u in users):
        raise HTTPException(status_code=400, detail="ç”¨æˆ·åå·²å­˜åœ¨")
    new_user = {
        "id": (max([u["id"] for u in users]) + 1) if users else 1,
        "username": payload.username,
        "email": payload.email or "",
        "role": payload.role or "æ™®é€šç”¨æˆ·",
        "password": payload.password or "",
        "position": payload.position or ""
    }
    users.append(new_user)
    return {"message": "ç”¨æˆ·åˆ›å»ºæˆåŠŸ", "user": _public_user(new_user)}

@app.put("/api/users/{user_id}")
async def update_user(user_id: int, payload: UserUpdate, ctx: Dict[str, Any] = Depends(require_auth)):
    user = next((u for u in users if u["id"] == user_id), None)
    if not user:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    if payload.email is not None:
        user["email"] = payload.email
    if payload.role is not None:
        user["role"] = payload.role
    if payload.password is not None and payload.password != "":
        user["password"] = payload.password
    if payload.position is not None:
        user["position"] = payload.position
    return {"message": "ç”¨æˆ·æ›´æ–°æˆåŠŸ", "user": _public_user(user)}

@app.delete("/api/users/{user_id}")
async def delete_user(user_id: int, ctx: Dict[str, Any] = Depends(require_auth)):
    global users
    before = len(users)
    users = [u for u in users if u["id"] != user_id]
    if len(users) == before:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    return {"message": "ç”¨æˆ·å·²åˆ é™¤"}

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    ) 